import os
from typing import Dict, List, Optional, Any, Union
import logging
from pathlib import Path
from pydantic import BaseModel
import numpy as np

import torch

from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoConfig
from transformers import AutoProcessor, AutoModelForImageTextToText

logger = logging.getLogger(__name__)

# æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ - æŒ‰æ˜¾å­˜éœ€æ±‚æ’åºï¼Œç”¨äºé™çº§ç­–ç•¥
SUPPORTED_MODELS = {
    # é€šç”¨å¤§æ¨¡å‹
    "Qwen/Qwen2.5-VL-7B-Instruct": {
        "type": "vl",
        "description": "é€šä¹‰åƒé—®2.5å¤šæ¨¡æ€å¤§æ¨¡å‹",
        "local_path": "models/Qwen2.5-VL-7B-Instruct",
        "device": "gpu",
        "min_gpu_memory": "14G",
        "memory_mb": 14336,
        "fallback_models": ["Qwen/Qwen2.5-3B-Instruct", "microsoft/DialoGPT-small"]
    },
    "Qwen/Qwen1.5-72B-Chat": {
        "type": "text",
        "description": "é€šä¹‰åƒé—®1.5-72Bå¯¹è¯æ¨¡å‹",
        "local_path": "models/Qwen1.5-72B-Chat",
        "device": "gpu",
        "min_gpu_memory": "72G",
        "memory_mb": 73728,
        "fallback_models": ["Qwen/Qwen2.5-7B-Instruct", "Qwen/Qwen2.5-3B-Instruct"]
    },
    "BAAI/bge-large-zh-v1.5": {
        "type": "embedding",
        "description": "ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹",
        "local_path": "models/bge-large-zh-v1.5",
        "device": "gpu",
        "min_gpu_memory": "2G",
        "memory_mb": 2048,
        "fallback_models": ["BAAI/bge-base-zh-v1.5", "BAAI/bge-small-zh-v1.5"]
    },
    "BAAI/bge-base-zh-v1.5": {
        "type": "embedding",
        "description": "è½»é‡çº§ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹",
        "local_path": "models/bge-base-zh-v1.5",
        "device": "gpu",
        "min_gpu_memory": "1G",
        "memory_mb": 1024,
        "fallback_models": ["BAAI/bge-small-zh-v1.5"]
    },
    # è½»é‡çº§é™çº§æ¨¡å‹
    "Qwen/Qwen2.5-3B-Instruct": {
        "type": "text",
        "description": "è½»é‡çº§é€šä¹‰åƒé—®æ¨¡å‹",
        "local_path": "models/Qwen2.5-3B-Instruct",
        "device": "gpu",
        "min_gpu_memory": "3G",
        "memory_mb": 3072,
        "fallback_models": ["microsoft/DialoGPT-small"]
    },
    "BAAI/bge-small-zh-v1.5": {
        "type": "embedding",
        "description": "å°å‹ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹",
        "local_path": "models/bge-small-zh-v1.5",
        "device": "cpu",
        "min_gpu_memory": "512M",
        "memory_mb": 512,
        "fallback_models": []
    },
    "microsoft/DialoGPT-small": {
        "type": "text",
        "description": "å°å‹å¯¹è¯æ¨¡å‹",
        "local_path": "models/DialoGPT-small",
        "device": "cpu",
        "min_gpu_memory": "1G",
        "memory_mb": 1024,
        "fallback_models": []
    }
}

class ModelRequest(BaseModel):
    """æ¨¡å‹è¯·æ±‚"""
    messages: List[Dict[str, Any]]  # æ”¯æŒå¤æ‚æ¶ˆæ¯æ ¼å¼
    max_tokens: int = 512
    temperature: float = 0.7
    image: Optional[str] = None

class ModelResponse(BaseModel):
    """æ¨¡å‹å“åº”"""
    content: str
    success: bool = True
    error: Optional[str] = None

class UnifiedModelManager:
    _instance = None
    _initialized = False
    _loaded_models = {}  # ç±»çº§åˆ«çš„æ¨¡å‹ç¼“å­˜
    _model_lock = None  # æ¨¡å‹åŠ è½½é”
    
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen2.5-VL-7B-Instruct",
        device_config: Optional[Dict[str, Any]] = None,
        offline_mode: bool = False
    ):
        """
        åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            device_config: è®¾å¤‡é…ç½®
            offline_mode: æ˜¯å¦ä½¿ç”¨ç¦»çº¿æ¨¡å¼
        """
        if self._initialized:
            return
            
        # åˆå§‹åŒ–çº¿ç¨‹é”
        import threading
        if self._model_lock is None:
            self._model_lock = threading.Lock()
            
        self.model_name = model_name
        self.device_config = device_config or {"gpu_ids": [0]}
        self.offline_mode = offline_mode
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.mock_mode = False
        self.model_type = "unknown"
        self.current_device = None
        self.available_gpu_memory = 0
        self.degraded_model = None
        self.current_loaded_model = None  # å½“å‰åŠ è½½çš„æ¨¡å‹åç§°
        
        # éªŒè¯æ¨¡å‹åç§°
        if model_name not in SUPPORTED_MODELS:
            logger.warning(f"æœªçŸ¥æ¨¡å‹: {model_name}ã€‚æ”¯æŒçš„æ¨¡å‹: {list(SUPPORTED_MODELS.keys())}")
        
        self._initialize()
        self._initialized = True
    
    def _get_model_path(self) -> str:
        """è·å–æ¨¡å‹è·¯å¾„"""
        model_info = SUPPORTED_MODELS.get(self.model_name, {})
        
        # å¦‚æœè®¾ç½®äº†ç¯å¢ƒå˜é‡ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„è·¯å¾„
        env_model_dir = os.getenv("ONCALL_MODEL_DIR")
        if env_model_dir:
            base_path = Path(env_model_dir)
        else:
            # å¦åˆ™ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„modelsç›®å½•
            base_path = Path(__file__).parent.parent.parent.parent / "models"
        
        model_path = base_path / model_info.get("local_path", self.model_name.split("/")[-1])
        
        if not model_path.exists() and self.offline_mode:
            logger.warning(
                f"ç¦»çº¿æ¨¡å¼ä¸‹æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹: {model_path}\n"
                f"è¯·ä¸‹è½½æ¨¡å‹åˆ°æŒ‡å®šç›®å½•ï¼Œæˆ–è®¾ç½®ONCALL_MODEL_DIRç¯å¢ƒå˜é‡æŒ‡å®šæ¨¡å‹ç›®å½•\n"
                f"ç³»ç»Ÿå°†å°è¯•ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ç»§ç»­è¿è¡Œ"
            )
            return None  # è¿”å›Noneè¡¨ç¤ºæ²¡æœ‰æ‰¾åˆ°æ¨¡å‹
        
        return str(model_path if model_path.exists() else self.model_name)
    
    def _get_available_gpu_memory(self) -> int:
        """è·å–å¯ç”¨GPUæ˜¾å­˜ï¼ˆMBï¼‰"""
        try:
            if torch.cuda.is_available():
                device_id = self.device_config.get("gpu_ids", [0])[0]
                torch.cuda.set_device(device_id)
                
                # è·å–GPUæ€»æ˜¾å­˜å’Œå·²ä½¿ç”¨æ˜¾å­˜
                total_memory = torch.cuda.get_device_properties(device_id).total_memory
                allocated_memory = torch.cuda.memory_allocated(device_id)
                reserved_memory = torch.cuda.memory_reserved(device_id)
                
                # è®¡ç®—å¯ç”¨æ˜¾å­˜ï¼ˆç•™å‡º1GBä½œä¸ºç¼“å†²ï¼‰
                available_memory = total_memory - max(allocated_memory, reserved_memory) - (1024 * 1024 * 1024)
                available_mb = max(0, available_memory // (1024 * 1024))
                
                logger.info(f"GPU {device_id} æ˜¾å­˜çŠ¶æ€: æ€»é‡={total_memory//1024//1024//1024:.1f}GB, "
                           f"å·²åˆ†é…={allocated_memory//1024//1024:.1f}MB, "
                           f"å·²é¢„ç•™={reserved_memory//1024//1024:.1f}MB, "
                           f"å¯ç”¨={available_mb:.1f}MB")
                
                return int(available_mb)
            else:
                logger.info("GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")
                return 0
        except Exception as e:
            logger.warning(f"GPUæ˜¾å­˜æ£€æµ‹å¤±è´¥: {e}")
            return 0
    
    def _predict_optimal_model(self, available_memory_mb: int) -> Optional[str]:
        """æ˜¾å­˜é¢„åˆ¤é€»è¾‘ï¼šæ ¹æ®å¯ç”¨æ˜¾å­˜é¢„æµ‹æœ€ä¼˜æ¨¡å‹ - ä¿®å¤æ¨¡å‹åŠ è½½æ•ˆç‡é—®é¢˜"""
        logger.info(f"ğŸ”® æ˜¾å­˜é¢„åˆ¤: å¯ç”¨æ˜¾å­˜ {available_memory_mb}MB")
        
        # æŒ‰æ˜¾å­˜éœ€æ±‚æ’åºæ‰€æœ‰æ”¯æŒçš„æ¨¡å‹
        sorted_models = sorted(
            SUPPORTED_MODELS.items(),
            key=lambda x: x[1].get('memory_mb', 0)
        )
        
        # æ‰¾åˆ°æœ€é€‚åˆçš„æ¨¡å‹ï¼ˆæ˜¾å­˜éœ€æ±‚ä¸è¶…è¿‡å¯ç”¨æ˜¾å­˜çš„80%ï¼Œç•™å‡ºå®‰å…¨è¾¹é™…ï¼‰
        safe_memory = int(available_memory_mb * 0.8)
        optimal_model = None
        
        for model_name, model_info in sorted_models:
            required_memory = model_info.get('memory_mb', 0)
            if required_memory <= safe_memory:
                optimal_model = model_name
                logger.debug(f"âœ“ å€™é€‰æ¨¡å‹: {model_name} (éœ€æ±‚: {required_memory}MB)")
            else:
                logger.debug(f"âœ— è·³è¿‡æ¨¡å‹: {model_name} (éœ€æ±‚: {required_memory}MB > å®‰å…¨é˜ˆå€¼: {safe_memory}MB)")
        
        if optimal_model:
            optimal_info = SUPPORTED_MODELS[optimal_model]
            logger.info(f"ğŸ¯ é¢„åˆ¤æœ€ä¼˜æ¨¡å‹: {optimal_model} (éœ€æ±‚: {optimal_info.get('memory_mb', 0)}MB)")
        else:
            logger.warning(f"âš ï¸ æ— åˆé€‚æ¨¡å‹ï¼Œå¯ç”¨æ˜¾å­˜: {available_memory_mb}MB")
        
        return optimal_model
    

    def _get_fallback_model(self, required_memory_mb: int) -> Optional[str]:
        """æ ¹æ®å¯ç”¨æ˜¾å­˜è·å–åˆé€‚çš„é™çº§æ¨¡å‹"""
        current_model_info = SUPPORTED_MODELS.get(self.model_name, {})
        fallback_models = current_model_info.get("fallback_models", [])
        
        # é¦–å…ˆæ£€æŸ¥å½“å‰æ¨¡å‹æ˜¯å¦é€‚åˆ
        current_memory_req = current_model_info.get("memory_mb", 0)
        if required_memory_mb >= current_memory_req:
            return self.model_name
        
        # æ£€æŸ¥é™çº§æ¨¡å‹
        for fallback_model in fallback_models:
            if fallback_model in SUPPORTED_MODELS:
                fallback_info = SUPPORTED_MODELS[fallback_model]
                fallback_memory_req = fallback_info.get("memory_mb", 0)
                if required_memory_mb >= fallback_memory_req:
                    logger.info(f"æ˜¾å­˜ä¸è¶³ï¼Œä» {self.model_name} é™çº§åˆ° {fallback_model}")
                    logger.info(f"éœ€æ±‚æ˜¾å­˜: {fallback_memory_req}MB, å¯ç”¨æ˜¾å­˜: {required_memory_mb}MB")
                    return fallback_model
        
        # å¦‚æœæ‰€æœ‰é™çº§æ¨¡å‹éƒ½ä¸é€‚åˆï¼Œè¿”å›æœ€å°çš„æ¨¡å‹æˆ–CPUæ¨¡å¼
        logger.warning(f"æ˜¾å­˜ä¸¥é‡ä¸è¶³ï¼ˆå¯ç”¨: {required_memory_mb}MBï¼‰ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼æˆ–æ¨¡æ‹Ÿæ¨¡å¼")
        return None

    
    def _initialize(self) -> None:
        """åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨ - ä¿®å¤æ¨¡å‹åŠ è½½æ•ˆç‡é—®é¢˜"""
        try:
            print("åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨")
            
            # æ£€æµ‹GPUå¯ç”¨æ€§å’Œæ˜¾å­˜
            self.available_gpu_memory = self._get_available_gpu_memory()
            
            # è®¾ç½®è®¾å¤‡
            if torch.cuda.is_available() and self.available_gpu_memory > 0:
                device_ids = self.device_config.get("gpu_ids", [0])
                device_id = device_ids[0]
                self.current_device = f"cuda:{device_id}"
                torch.cuda.set_device(device_id)
                logger.info(f"ä½¿ç”¨GPUè®¾å¤‡: {self.current_device}, å¯ç”¨æ˜¾å­˜: {self.available_gpu_memory}MB")
            else:
                self.current_device = "cpu"
                logger.info("ä½¿ç”¨CPUæ¨¡å¼")
            
            # æ˜¾å­˜é¢„åˆ¤é€»è¾‘ï¼šæ ¹æ®å‰©ä½™æ˜¾å­˜ç›´æ¥é€‰æ‹©åˆé€‚æ¨¡å‹
            if self.current_device.startswith("cuda"):
                optimal_model = self._predict_optimal_model(self.available_gpu_memory)
                if optimal_model and optimal_model != self.model_name:
                    logger.warning(f"âš™ï¸ æ˜¾å­˜é¢„åˆ¤ä¼˜åŒ–: åŸæ¨¡å‹ {self.model_name} éœ€è¦ {SUPPORTED_MODELS.get(self.model_name, {}).get('memory_mb', 0)}MB æ˜¾å­˜")
                    logger.warning(f"âš™ï¸ å½“å‰å¯ç”¨æ˜¾å­˜: {self.available_gpu_memory}MBï¼Œé¢„åˆ¤æœ€ä¼˜æ¨¡å‹: {optimal_model}")
                    self.degraded_model = self.model_name
                    self.model_name = optimal_model
                elif optimal_model is None:
                    logger.warning(f"âš ï¸ æ˜¾å­˜ä¸¥é‡ä¸è¶³ï¼åŸæ¨¡å‹ {self.model_name} éœ€è¦ {SUPPORTED_MODELS.get(self.model_name, {}).get('memory_mb', 0)}MB")
                    logger.warning(f"âš ï¸ å½“å‰å¯ç”¨æ˜¾å­˜: {self.available_gpu_memory}MBï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
                    self.current_device = "cpu"
            
            # ä¸åœ¨åˆå§‹åŒ–æ—¶åŠ è½½æ¨¡å‹ï¼Œé‡‡ç”¨æ‡’åŠ è½½ç­–ç•¥
            logger.info("âœ¨ æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œé‡‡ç”¨ä¸²è¡Œæ‡’åŠ è½½ç­–ç•¥")
            logger.info(f"ğŸ“Š GPUæ˜¾å­˜çŠ¶æ€: å¯ç”¨{self.available_gpu_memory}MB")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.model = None
            self.tokenizer = None
            self.mock_mode = True
    
    def _release_current_model(self) -> None:
        """é‡Šæ”¾å½“å‰åŠ è½½çš„æ¨¡å‹ï¼Œæ¸…ç†GPUæ˜¾å­˜"""
        try:
            if self.model is not None:
                logger.info(f"ğŸ§¹ é‡Šæ”¾æ¨¡å‹: {self.current_loaded_model}")
                
                # å°†æ¨¡å‹ç§»åˆ°CPUå¹¶åˆ é™¤å¼•ç”¨
                if hasattr(self.model, 'to'):
                    self.model = self.model.to('cpu')
                del self.model
                self.model = None
                
                # é‡Šæ”¾tokenizerå’Œprocessor
                if self.tokenizer is not None:
                    del self.tokenizer
                    self.tokenizer = None
                    
                if self.processor is not None:
                    del self.processor
                    self.processor = None
                
                # æ¸…ç†GPUç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                
                # æ›´æ–°çŠ¶æ€
                self.current_loaded_model = None
                self.model_type = "unknown"
                
                # é‡æ–°æ£€æµ‹å¯ç”¨æ˜¾å­˜
                self.available_gpu_memory = self._get_available_gpu_memory()
                logger.info(f"âœ… æ¨¡å‹é‡Šæ”¾å®Œæˆï¼Œå¯ç”¨æ˜¾å­˜: {self.available_gpu_memory}MB")
                
        except Exception as e:
            logger.warning(f"æ¨¡å‹é‡Šæ”¾è¿‡ç¨‹ä¸­å‡ºç°è­¦å‘Š: {e}")
    
    def _ensure_model_loaded(self, required_model: str) -> bool:
        """ç¡®ä¿æŒ‡å®šæ¨¡å‹å·²åŠ è½½ï¼ˆä¸²è¡ŒåŠ è½½ç­–ç•¥ï¼‰"""
        with self._model_lock:
            try:
                # å¦‚æœå½“å‰å·²åŠ è½½æ‰€éœ€æ¨¡å‹ï¼Œç›´æ¥è¿”å›
                if (self.current_loaded_model == required_model and 
                    self.model is not None and not self.mock_mode):
                    logger.debug(f"âœ… æ¨¡å‹ {required_model} å·²åŠ è½½")
                    return True
                
                # å¦‚æœåŠ è½½äº†å…¶ä»–æ¨¡å‹ï¼Œå…ˆé‡Šæ”¾
                if self.current_loaded_model and self.current_loaded_model != required_model:
                    logger.info(f"ğŸ”„ åˆ‡æ¢æ¨¡å‹: {self.current_loaded_model} -> {required_model}")
                    self._release_current_model()
                
                # åŠ è½½æ–°æ¨¡å‹
                logger.info(f"â³ ä¸²è¡ŒåŠ è½½æ¨¡å‹: {required_model}")
                
                # ä¸´æ—¶åˆ‡æ¢æ¨¡å‹åç§°
                original_model_name = self.model_name
                self.model_name = required_model
                
                # é‡æ–°æ£€æµ‹å¯ç”¨æ˜¾å­˜
                self.available_gpu_memory = self._get_available_gpu_memory()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é™çº§
                if self.current_device.startswith("cuda"):
                    fallback_model = self._get_fallback_model(self.available_gpu_memory)
                    if fallback_model and fallback_model != required_model:
                        logger.warning(f"âš™ï¸ æ˜¾å­˜ä¸è¶³ï¼Œä» {required_model} é™çº§åˆ° {fallback_model}")
                        self.model_name = fallback_model
                        required_model = fallback_model
                    elif fallback_model is None:
                        logger.warning(f"âš ï¸ æ˜¾å­˜ä¸è¶³ï¼Œåˆ‡æ¢åˆ°CPUæ¨¡å¼")
                        self.current_device = "cpu"
                
                # å°è¯•åŠ è½½æ¨¡å‹
                success = False
                is_vl_model = "VL" in self.model_name or "vision" in self.model_name.lower()
                
                if is_vl_model:
                    success = self._load_vl_model_with_fallback()
                else:
                    success = self._load_standard_model_with_fallback()
                
                if success:
                    self.current_loaded_model = self.model_name
                    logger.info(f"âœ… æ¨¡å‹ {self.model_name} åŠ è½½æˆåŠŸ")
                    return True
                else:
                    logger.warning(f"âŒ æ¨¡å‹ {required_model} åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                    self.mock_mode = True
                    self.current_loaded_model = required_model
                    return False
                    
            except Exception as e:
                logger.error(f"ä¸²è¡Œæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.mock_mode = True
                return False
            finally:
                # æ¢å¤åŸå§‹æ¨¡å‹åç§°ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if 'original_model_name' in locals():
                    self.model_name = original_model_name
    
    def _load_vl_model_with_fallback(self) -> bool:
        """å¸¦æœ‰é™çº§ç­–ç•¥çš„VLæ¨¡å‹åŠ è½½"""
        try:
            return self._load_vl_model(self.current_device)
        except RuntimeError as e:
            if "CUDA out of memory" in str(e) or "out of memory" in str(e).lower():
                logger.error(f"âš ï¸ CUDAæ˜¾å­˜ä¸è¶³é”™è¯¯: {e}")
                return self._handle_oom_fallback()
            else:
                logger.error(f"VLæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                return False
        except Exception as e:
            logger.error(f"VLæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _load_standard_model_with_fallback(self) -> bool:
        """å¸¦æœ‰é™çº§ç­–ç•¥çš„æ ‡å‡†æ¨¡å‹åŠ è½½"""
        try:
            return self._load_standard_model()
        except RuntimeError as e:
            if "CUDA out of memory" in str(e) or "out of memory" in str(e).lower():
                logger.error(f"âš ï¸ CUDAæ˜¾å­˜ä¸è¶³é”™è¯¯: {e}")
                return self._handle_oom_fallback()
            else:
                logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                return False
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _handle_oom_fallback(self) -> bool:
        """å¤„ç†æ˜¾å­˜ä¸è¶³çš„é™çº§ç­–ç•¥"""
        logger.warning(f"ğŸ”„ æ­£åœ¨å°è¯•æ˜¾å­˜ä¸è¶³é™çº§ç­–ç•¥...")
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜")
        
        # é‡æ–°æ£€æµ‹å¯ç”¨æ˜¾å­˜
        self.available_gpu_memory = self._get_available_gpu_memory()
        
        # å°è¯•é™çº§æ¨¡å‹
        current_model_info = SUPPORTED_MODELS.get(self.model_name, {})
        fallback_models = current_model_info.get("fallback_models", [])
        
        for fallback_model in fallback_models:
            if fallback_model in SUPPORTED_MODELS:
                fallback_info = SUPPORTED_MODELS[fallback_model]
                fallback_memory_req = fallback_info.get("memory_mb", 0)
                
                if self.available_gpu_memory >= fallback_memory_req:
                    logger.warning(f"ğŸ”„ å°è¯•é™çº§åˆ°æ¨¡å‹: {fallback_model} (éœ€æ±‚æ˜¾å­˜: {fallback_memory_req}MB)")
                    
                    # ä¿å­˜åŸå§‹æ¨¡å‹åç§°
                    if not self.degraded_model:
                        self.degraded_model = self.model_name
                    
                    # åˆ‡æ¢åˆ°é™çº§æ¨¡å‹
                    self.model_name = fallback_model
                    
                    # å°è¯•åŠ è½½é™çº§æ¨¡å‹
                    try:
                        if "VL" in fallback_model or "vision" in fallback_model.lower():
                            success = self._load_vl_model(self.current_device)
                        else:
                            success = self._load_standard_model()
                        
                        if success:
                            logger.warning(f"âœ… é™çº§æˆåŠŸï¼å½“å‰ä½¿ç”¨æ¨¡å‹: {fallback_model}")
                            return True
                    except Exception as e:
                        logger.warning(f"é™çº§æ¨¡å‹ {fallback_model} åŠ è½½ä¹Ÿå¤±è´¥: {e}")
                        continue
        
        # å¦‚æœæ‰€æœ‰é™çº§æ¨¡å‹éƒ½å¤±è´¥ï¼Œå°è¯•CPUæ¨¡å¼
        logger.warning(f"ğŸ’» æ‰€æœ‰GPUæ¨¡å‹éƒ½æ— æ³•åŠ è½½ï¼Œå°è¯•CPUæ¨¡å¼")
        self.current_device = "cpu"
        
        try:
            if self._load_standard_model():
                logger.warning(f"âœ… CPUæ¨¡å¼åŠ è½½æˆåŠŸï¼")
                return True
        except Exception as e:
            logger.error(f"CPUæ¨¡å¼ä¹Ÿå¤±è´¥: {e}")
        
        # æœ€åé™çº§åˆ°æ¨¡æ‹Ÿæ¨¡å¼
        logger.warning(f"ğŸ­ æ‰€æœ‰åŠ è½½å°è¯•éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
        self.mock_mode = True
        return False
    
    def _load_standard_model(self) -> bool:
        """åŠ è½½æ ‡å‡†æ¨¡å‹"""
        model_path = self._get_model_path()
        
        # å¦‚æœæ¨¡å‹è·¯å¾„ä¸ºNoneï¼Œè¯´æ˜ç¦»çº¿æ¨¡å¼ä¸‹æ‰¾ä¸åˆ°æœ¬åœ°æ¨¡å‹
        if model_path is None:
            logger.warning(f"æ¨¡å‹ {self.model_name} æ— æ³•åœ¨ç¦»çº¿æ¨¡å¼ä¸‹åŠ è½½")
            return False
        
        # åŠ è½½åˆ†è¯å™¨
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                local_files_only=self.offline_mode
            )
            logger.info(f"æˆåŠŸåŠ è½½åˆ†è¯å™¨: {model_path}")
        except Exception as e:
            logger.error(f"åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
            return False
        
        # åŠ è½½æ¨¡å‹ - å°è¯•å¤šç§åŠ è½½æ–¹å¼
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨AutoModelForCausalLM
            load_kwargs = {
                "trust_remote_code": True,
                "local_files_only": self.offline_mode
            }
            
            # æ ¹æ®è®¾å¤‡è®¾ç½®åŠ è½½å‚æ•°
            if self.current_device.startswith("cuda"):
                load_kwargs["device_map"] = self.current_device
                load_kwargs["torch_dtype"] = torch.float16  # ä½¿ç”¨åŠç²¾åº¦èŠ‚çœæ˜¾å­˜
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                **load_kwargs
            )
            self.model_type = "causal_lm"
            logger.info(f"æˆåŠŸä½¿ç”¨AutoModelForCausalLMåŠ è½½æ¨¡å‹: {self.model_name}")
            
        except Exception as e:
            logger.warning(f"AutoModelForCausalLMåŠ è½½å¤±è´¥: {e}")
            try:
                # å°è¯•ä½¿ç”¨AutoModel
                load_kwargs = {
                    "trust_remote_code": True,
                    "local_files_only": self.offline_mode
                }
                
                if self.current_device.startswith("cuda"):
                    load_kwargs["device_map"] = self.current_device
                    load_kwargs["torch_dtype"] = torch.float16
                
                self.model = AutoModel.from_pretrained(
                    model_path,
                    **load_kwargs
                )
                self.model_type = "base_model"
                logger.info(f"æˆåŠŸä½¿ç”¨AutoModelåŠ è½½æ¨¡å‹: {self.model_name}")
                
            except Exception as e2:
                logger.error(f"æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥: {e2}")
                raise e2  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚å¤„ç†
        
        # å¦‚æœæˆåŠŸåŠ è½½ä¸”ä½¿ç”¨CPUæ¨¡å¼ï¼Œå°†æ¨¡å‹ç§»åŠ¨åˆ°CPU
        if self.model and self.current_device == "cpu":
            self.model = self.model.to("cpu")
        
        if self.model:
            self.mock_mode = False
            logger.info(f"æ¨¡å‹ {self.model_name} åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.current_device}")
            return True
        
        return False
    
    def _load_vl_model(self, device: str) -> bool:
        """ä¸“é—¨ç”¨äºåŠ è½½VLæ¨¡å‹çš„æ–¹æ³•"""
        try:
            # é¦–å…ˆå°è¯•åœ¨çº¿åŠ è½½processorå’Œæ¨¡å‹
            if not self.offline_mode:
                logger.info("åœ¨çº¿æ¨¡å¼åŠ è½½VLæ¨¡å‹")
                
                # æ¸è¿›å¼åŠ è½½ç­–ç•¥ï¼šå…ˆåŠ è½½processorï¼Œå†åŠ è½½æ¨¡å‹
                try:
                    logger.info(f"æ­¥éª¤1: åŠ è½½processor for {self.model_name}")
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæµ‹è¯•ç¯å¢ƒæˆ–èµ„æºå—é™ç¯å¢ƒ
                    import psutil
                    available_memory = psutil.virtual_memory().available // (1024**3)  # GB
                    
                    if available_memory < 8:  # å¦‚æœå¯ç”¨å†…å­˜å°äº8GB
                        logger.warning(f"âš ï¸ å¯ç”¨å†…å­˜ä¸è¶³ ({available_memory}GB < 8GB)ï¼Œè·³è¿‡å¤§æ¨¡å‹åŠ è½½ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                        self.mock_mode = True
                        return False
                    
                    self.processor = AutoProcessor.from_pretrained(
                        self.model_name,
                        trust_remote_code=True
                    )
                    logger.info("âœ… ProcessoråŠ è½½æˆåŠŸ")
                except ImportError:
                    # å¦‚æœæ²¡æœ‰psutilï¼Œç»§ç»­æ­£å¸¸åŠ è½½ä½†æ·»åŠ è­¦å‘Š
                    logger.warning("psutilæœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹å†…å­˜çŠ¶æ€ï¼Œç»§ç»­å°è¯•åŠ è½½æ¨¡å‹")
                    try:
                        self.processor = AutoProcessor.from_pretrained(
                            self.model_name,
                            trust_remote_code=True
                        )
                        logger.info("âœ… ProcessoråŠ è½½æˆåŠŸ")
                    except Exception as e:
                        logger.error(f"ProcessoråŠ è½½å¤±è´¥: {e}")
                        return False
                except Exception as e:
                    logger.error(f"ProcessoråŠ è½½å¤±è´¥: {e}")
                    return False
                
                # å°è¯•å¤šç§VLæ¨¡å‹åŠ è½½æ–¹å¼
                load_kwargs = {
                    "trust_remote_code": True,
                    "torch_dtype": torch.float16,  # ä½¿ç”¨åŠç²¾åº¦èŠ‚çœæ˜¾å­˜
                    "low_cpu_mem_usage": True  # ä¼˜åŒ–CPUå†…å­˜ä½¿ç”¨
                }
                
                # æ ¹æ®è®¾å¤‡è®¾ç½®åŠ è½½å‚æ•°
                if device.startswith("cuda"):
                    load_kwargs["device_map"] = device
                
                try:
                    logger.info(f"æ­¥éª¤2: å°è¯•ä½¿ç”¨AutoModelForImageTextToTextåŠ è½½ {self.model_name}")
                    # æ–¹å¼1: AutoModelForImageTextToText
                    self.model = AutoModelForImageTextToText.from_pretrained(
                        self.model_name,
                        **load_kwargs
                    )
                    self.model_type = "vl_model"
                    logger.info("âœ… æˆåŠŸä½¿ç”¨AutoModelForImageTextToTextåŠ è½½VLæ¨¡å‹")
                    return True
                    
                except Exception as e1:
                    logger.warning(f"AutoModelForImageTextToTextåŠ è½½å¤±è´¥: {e1}")
                    
                    # å¦‚æœæ˜¯æ˜¾å­˜ä¸è¶³é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    if "CUDA out of memory" in str(e1) or "out of memory" in str(e1).lower():
                        raise e1
                    
                    try:
                        logger.info(f"æ­¥éª¤3: å°è¯•ä½¿ç”¨AutoModelForCausalLMåŠ è½½ {self.model_name}")
                        # æ–¹å¼2: AutoModelForCausalLM
                        self.model = AutoModelForCausalLM.from_pretrained(
                            self.model_name,
                            **load_kwargs
                        )
                        self.model_type = "vl_causal_lm"
                        logger.info("âœ… æˆåŠŸä½¿ç”¨AutoModelForCausalLMåŠ è½½VLæ¨¡å‹")
                        return True
                        
                    except Exception as e2:
                        logger.error(f"æ‰€æœ‰VLæ¨¡å‹åŠ è½½æ–¹å¼éƒ½å¤±è´¥: {e2}")
                        
                        # å¦‚æœæ˜¯æ˜¾å­˜ä¸è¶³é”™è¯¯ï¼ŒæŠ›å‡ºä»¥ä¾¿ä¸Šå±‚å¤„ç†
                        if "CUDA out of memory" in str(e2) or "out of memory" in str(e2).lower():
                            raise e2
                        
                        return False
            else:
                # ç¦»çº¿æ¨¡å¼ä¸‹ï¼ŒVLæ¨¡å‹æ¯”è¾ƒå¤æ‚ï¼Œç›´æ¥è¿›å…¥æ¨¡æ‹Ÿæ¨¡å¼
                logger.warning("ç¦»çº¿æ¨¡å¼ä¸‹æš‚ä¸æ”¯æŒVLæ¨¡å‹åŠ è½½ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                return False
                
        except Exception as e:
            logger.error(f"VLæ¨¡å‹åŠ è½½è¿‡ç¨‹å‡ºé”™: {e}")
            # å¦‚æœæ˜¯æ˜¾å­˜ä¸è¶³é”™è¯¯ï¼ŒæŠ›å‡ºä»¥ä¾¿ä¸Šå±‚å¤„ç†
            if "CUDA out of memory" in str(e) or "out of memory" in str(e).lower():
                raise e
            return False
    
    async def generate(self, request: ModelRequest) -> ModelResponse:
        """
        ç”Ÿæˆå›ç­”
        
        Args:
            request: è¯·æ±‚å‚æ•°
        
        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        try:
            # ä¸²è¡ŒåŠ è½½ç­–ç•¥ï¼šç¡®ä¿æ‰€éœ€æ¨¡å‹å·²åŠ è½½
            if not self._ensure_model_loaded(self.model_name):
                # å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
                query_content = self._extract_query_content(request.messages)
                return ModelResponse(
                    content=f"æ¨¡æ‹Ÿæ¨¡å¼å›ç­”ï¼šåŸºäºæŸ¥è¯¢ '{query_content[:50]}...'ï¼Œå»ºè®®å‡çº§ç¯å¢ƒä»¥ä½¿ç”¨çœŸå®æ¨¡å‹ã€‚",
                    success=True
                )
            
            # æ¨¡æ‹Ÿæ¨¡å¼ç®€å•å¤„ç†
            if hasattr(self, 'mock_mode') and self.mock_mode:
                query_content = self._extract_query_content(request.messages)
                return ModelResponse(
                    content=f"æ¨¡æ‹Ÿæ¨¡å¼å›ç­”ï¼šåŸºäºæŸ¥è¯¢ '{query_content[:50]}...'ï¼Œå»ºè®®å‡çº§ç¯å¢ƒä»¥ä½¿ç”¨çœŸå®æ¨¡å‹ã€‚",
                    success=True
                )
            
            if not self.model:
                raise RuntimeError("æ¨¡å‹æœªæ­£ç¡®åˆå§‹åŒ–")
            
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ç”Ÿæˆæ–¹å¼
            if self.model_type == "vl_model" and hasattr(self, 'processor') and self.processor:
                return await self._generate_with_vl_processor(request)
            elif self.model_type in ["causal_lm", "vl_causal_lm"] and hasattr(self.model, 'generate'):
                return await self._generate_with_causal_lm(request)
            elif hasattr(self, 'tokenizer') and self.tokenizer:
                return await self._generate_with_base_model(request)
            else:
                raise RuntimeError("ç¼ºå°‘åˆé€‚çš„ç”Ÿæˆæ–¹æ³•")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤±è´¥: {e}")
            return ModelResponse(
                content="",
                success=False,
                error=str(e)
            )
    
    def release_model_after_use(self) -> None:
        """ä»»åŠ¡å®Œæˆåé‡Šæ”¾æ¨¡å‹ï¼Œä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨"""
        try:
            if self.current_loaded_model:
                logger.info(f"ğŸ† ä»»åŠ¡å®Œæˆï¼Œè‡ªåŠ¨é‡Šæ”¾æ¨¡å‹: {self.current_loaded_model}")
                self._release_current_model()
        except Exception as e:
            logger.warning(f"ä»»åŠ¡å®Œæˆåæ¨¡å‹é‡Šæ”¾å¤±è´¥: {e}")
    
    @classmethod
    def get_gpu_memory_status(cls) -> Dict[str, Any]:
        """è·å–GPUæ˜¾å­˜çŠ¶æ€ä¿¡æ¯"""
        try:
            if torch.cuda.is_available():
                device_count = torch.cuda.device_count()
                status = {
                    "gpu_available": True,
                    "device_count": device_count,
                    "devices": []
                }
                
                for i in range(device_count):
                    props = torch.cuda.get_device_properties(i)
                    total_memory = props.total_memory
                    allocated_memory = torch.cuda.memory_allocated(i)
                    reserved_memory = torch.cuda.memory_reserved(i)
                    free_memory = total_memory - max(allocated_memory, reserved_memory)
                    
                    device_info = {
                        "device_id": i,
                        "name": props.name,
                        "total_memory_gb": round(total_memory / (1024**3), 2),
                        "allocated_memory_mb": round(allocated_memory / (1024**2), 2),
                        "reserved_memory_mb": round(reserved_memory / (1024**2), 2),
                        "free_memory_mb": round(free_memory / (1024**2), 2),
                        "utilization_percent": round((allocated_memory / total_memory) * 100, 2)
                    }
                    status["devices"].append(device_info)
                
                return status
            else:
                return {
                    "gpu_available": False,
                    "message": "CUDAä¸å¯ç”¨æˆ–æœªå®‰è£…"
                }
        except Exception as e:
            return {
                "gpu_available": False,
                "error": str(e)
            }
    
    async def _generate_with_vl_processor(self, request: ModelRequest) -> ModelResponse:
        """ä½¿ç”¨VLä¸“ç”¨processorç”Ÿæˆå“åº”"""
        try:
            # æå–æ–‡æœ¬å†…å®¹
            text_content = self._extract_query_content(request.messages)
            image_content = None
            
            # å¤„ç†å›¾åƒ
            if request.image:
                image_content = await self._load_image(request.image)
                if image_content:
                    logger.info(f"æˆåŠŸåŠ è½½å›¾åƒ: {image_content.size}")
            
            # ä¸ºQwen2.5-VLæ„å»ºæ­£ç¡®çš„è¾“å…¥æ ¼å¼
            if image_content:
                # ä½¿ç”¨processorçš„chatæ¨¡æ¿åŠŸèƒ½
                try:
                    # æ„å»ºæ ‡å‡†çš„å¯¹è¯æ¶ˆæ¯
                    conversation = [
                        {
                            "role": "user",
                            "content": [
                                {"type": "image", "image": image_content},
                                {"type": "text", "text": text_content}
                            ]
                        }
                    ]
                    
                    # ä½¿ç”¨apply_chat_templateå¤„ç†å¯¹è¯
                    if hasattr(self.processor.tokenizer, 'apply_chat_template'):
                        text_prompt = self.processor.tokenizer.apply_chat_template(
                            conversation, 
                            tokenize=False, 
                            add_generation_prompt=True
                        )
                    else:
                        # å›é€€åˆ°ç®€å•çš„æ–‡æœ¬å¤„ç†
                        text_prompt = f"<|im_start|>user\n{text_content}<|im_end|>\n<|im_start|>assistant\n"
                    
                    # ä½¿ç”¨processorå¤„ç†å›¾åƒå’Œæ–‡æœ¬
                    inputs = self.processor(
                        text=text_prompt,
                        images=image_content,
                        return_tensors="pt"
                    ).to(self.model.device)
                    
                    logger.info("VLå¤šæ¨¡æ€è¾“å…¥ï¼šä½¿ç”¨chatæ¨¡æ¿æ ¼å¼")
                    
                except Exception as e:
                    logger.warning(f"Chatæ¨¡æ¿å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æ ¼å¼: {e}")
                    # å›é€€åˆ°ç®€åŒ–å¤„ç†
                    inputs = self.processor(
                        text=text_content,
                        images=image_content,
                        return_tensors="pt"
                    ).to(self.model.device)
                    logger.info("VLå¤šæ¨¡æ€è¾“å…¥ï¼šä½¿ç”¨ç®€åŒ–æ ¼å¼")
            else:
                # çº¯æ–‡æœ¬è¾“å…¥
                inputs = self.processor(
                    text=text_content,
                    return_tensors="pt"
                ).to(self.model.device)
                logger.info("VLçº¯æ–‡æœ¬è¾“å…¥")
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                if hasattr(self.model, 'generate'):
                    # ä½¿ç”¨è¾ƒå°çš„å‚æ•°é¿å…tokenä¸åŒ¹é…é—®é¢˜
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=min(request.max_tokens, 256),  # é™åˆ¶ç”Ÿæˆé•¿åº¦
                        temperature=request.temperature,
                        do_sample=True if request.temperature > 0 else False,
                        pad_token_id=getattr(self.processor.tokenizer, 'eos_token_id', None),
                        use_cache=True  # å¯ç”¨ç¼“å­˜
                    )
                    
                    # è§£ç è¾“å‡º
                    if hasattr(self.processor, 'batch_decode'):
                        # ä½¿ç”¨batch_decodeå¤„ç†æ•´ä¸ªè¾“å‡º
                        full_response = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]
                        # æå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
                        input_text = self.processor.batch_decode(inputs.input_ids, skip_special_tokens=True)[0]
                        if input_text in full_response:
                            response = full_response[len(input_text):].strip()
                        else:
                            response = full_response
                    else:
                        # ä¼ ç»Ÿæ–¹å¼ï¼šåªè§£ç æ–°ç”Ÿæˆçš„token
                        generated_ids = outputs[0][len(inputs.input_ids[0]):]
                        response = self.processor.decode(generated_ids, skip_special_tokens=True)
                    
                    return ModelResponse(
                        content=response.strip(),
                        success=True
                    )
                else:
                    return ModelResponse(
                        content="VLæ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½†ç¼ºå°‘generateæ–¹æ³•ã€‚è¯·æ£€æŸ¥æ¨¡å‹ç‰ˆæœ¬ã€‚",
                        success=False,
                        error="Missing generate method"
                    )
            
        except Exception as e:
            logger.error(f"VL Processorç”Ÿæˆå¤±è´¥: {e}")
            # å¦‚æœæ˜¯tokenä¸åŒ¹é…é”™è¯¯ï¼Œå°è¯•é™çº§å¤„ç†
            if "features and image tokens do not match" in str(e):
                logger.info("æ£€æµ‹åˆ°tokenä¸åŒ¹é…é”™è¯¯ï¼Œå°è¯•ç®€åŒ–å¤„ç†...")
                return await self._fallback_vl_generation(request)
            
            return ModelResponse(
                content="",
                success=False,
                error=str(e)
            )
    
    async def _fallback_vl_generation(self, request: ModelRequest) -> ModelResponse:
        """VLæ¨¡å‹çš„å›é€€ç”Ÿæˆæ–¹æ³•"""
        try:
            # æå–æŸ¥è¯¢æ–‡æœ¬
            text_content = self._extract_query_content(request.messages)
            
            # å¦‚æœæœ‰å›¾åƒï¼Œå°è¯•æè¿°å›¾åƒ
            image_description = ""
            if request.image:
                image_content = await self._load_image(request.image)
                if image_content:
                    image_description = f"ï¼ˆå›¾åƒå°ºå¯¸: {image_content.size}ï¼Œæ ¼å¼: {image_content.format}ï¼‰"
            
            # è¿”å›æè¿°æ€§å›ç­”
            fallback_content = f"""å›¾åƒåˆ†æè¯·æ±‚å·²æ¥æ”¶ã€‚{image_description}

æŸ¥è¯¢: {text_content}

ç”±äºå½“å‰æ¨¡å‹é…ç½®é™åˆ¶ï¼Œæ— æ³•ç›´æ¥åˆ†æå›¾åƒå†…å®¹ã€‚å»ºè®®ï¼š
1. æ£€æŸ¥transformersç‰ˆæœ¬æ˜¯å¦ä¸ºæœ€æ–°
2. ç¡®è®¤Qwen2.5-VLæ¨¡å‹é…ç½®æ­£ç¡®
3. æˆ–è€…æä¾›å›¾åƒçš„æ–‡å­—æè¿°ä»¥ä¾¿åˆ†æ

æ¨¡æ‹Ÿåˆ†æï¼šå¦‚æœå›¾åƒåŒ…å«æŠ€æœ¯é—®é¢˜ã€é”™è¯¯ä¿¡æ¯æˆ–éœ€è¦è§£é‡Šçš„å†…å®¹ï¼Œè¯·è¯¦ç»†æè¿°å›¾åƒä¸­çš„æ–‡å­—å’Œå…³é”®ä¿¡æ¯ï¼Œæˆ‘å°†åŸºäºæè¿°æä¾›å¸®åŠ©ã€‚"""
            
            return ModelResponse(
                content=fallback_content,
                success=True
            )
            
        except Exception as e:
            logger.error(f"å›é€€ç”Ÿæˆä¹Ÿå¤±è´¥: {e}")
            return ModelResponse(
                content=f"å›¾åƒåˆ†æåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨: {str(e)}",
                success=False,
                error=str(e)
            )
    
    async def _generate_with_causal_lm(self, request: ModelRequest) -> ModelResponse:
        """ä½¿ç”¨CausalLMæ¨¡å‹ç”Ÿæˆå“åº”"""
        try:
            messages = request.messages
            
            # ä½¿ç”¨tokenizeræˆ–processorå¤„ç†è¾“å…¥
            if hasattr(self, 'processor') and self.processor:
                # å¯¹äºVL CausalLMï¼Œä½¿ç”¨processor
                text_content = self._extract_query_content(messages)
                image_content = None
                
                if request.image:
                    image_content = await self._load_image(request.image)
                
                if image_content:
                    inputs = self.processor(
                        text=text_content,
                        images=image_content,
                        return_tensors="pt"
                    ).to(self.model.device)
                else:
                    inputs = self.processor(
                        text=text_content,
                        return_tensors="pt"
                    ).to(self.model.device)
            else:
                # å¯¹äºçº¯æ–‡æœ¬CausalLMï¼Œä½¿ç”¨tokenizer
                if hasattr(self.tokenizer, "apply_chat_template"):
                    prompt = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                else:
                    prompt = "\n".join([f"{msg['role']}: {msg['content']}" for msg in messages])
                
                inputs = self.tokenizer(
                    prompt,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=2048
                ).to(self.model.device)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=request.max_tokens,
                    temperature=request.temperature,
                    do_sample=True if request.temperature > 0 else False,
                    pad_token_id=getattr(self.tokenizer, 'eos_token_id', None) or getattr(self.processor.tokenizer, 'eos_token_id', None) if hasattr(self, 'processor') else None
                )
            
            # è§£ç è¾“å‡º
            if hasattr(self, 'processor') and self.processor:
                generated_ids = outputs[0][len(inputs.input_ids[0]):]
                response = self.processor.decode(generated_ids, skip_special_tokens=True)
            else:
                response = self.tokenizer.decode(
                    outputs[0][len(inputs.input_ids[0]):],
                    skip_special_tokens=True
                )
            
            return ModelResponse(
                content=response.strip(),
                success=True
            )
            
        except Exception as e:
            logger.error(f"CausalLMç”Ÿæˆå¤±è´¥: {e}")
            return ModelResponse(
                content="",
                success=False,
                error=str(e)
            )
    
    async def _generate_with_base_model(self, request: ModelRequest) -> ModelResponse:
        """ä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆå“åº”ï¼ˆæ²¡æœ‰generateæ–¹æ³•çš„æ¨¡å‹ï¼‰"""
        try:
            # å¯¹äºæ²¡æœ‰generateæ–¹æ³•çš„æ¨¡å‹ï¼Œæˆ‘ä»¬åªèƒ½è¿”å›ä¸€ä¸ªè¯´æ˜
            logger.warning("åŸºç¡€æ¨¡å‹æ²¡æœ‰generateæ–¹æ³•ï¼Œæ— æ³•ç”Ÿæˆå›ç­”")
            
            query_content = self._extract_query_content(request.messages)
            
            return ModelResponse(
                content=f"æ¨¡å‹å·²åŠ è½½ä½†ç¼ºå°‘generateæ–¹æ³•ã€‚æ— æ³•å¯¹æŸ¥è¯¢ '{query_content[:50]}...' ç”Ÿæˆå›ç­”ã€‚è¯·ä½¿ç”¨æ”¯æŒç”Ÿæˆçš„æ¨¡å‹ç‰ˆæœ¬ã€‚",
                success=False,
                error="Model does not support generation"
            )
            
        except Exception as e:
            logger.error(f"åŸºç¡€æ¨¡å‹å¤„ç†å¤±è´¥: {e}")
            return ModelResponse(
                content="",
                success=False,
                error=str(e)
            )
    
    async def _load_image(self, image_source):
        """
        åŠ è½½å›¾åƒï¼Œæ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ï¼š
        - URL (http/https)
        - æœ¬åœ°æ–‡ä»¶è·¯å¾„
        - base64 å­—ç¬¦ä¸² (çº¯ base64 æˆ– data:image/xxx;base64,...)
        - PIL Image å¯¹è±¡
        """
        try:
            from PIL import Image
            import base64
            from io import BytesIO
            import os
            
            if image_source is None:
                return None
            
            # å¦‚æœå·²ç»æ˜¯ PIL Image å¯¹è±¡
            if hasattr(image_source, 'save'):
                return image_source.convert('RGB')
            
            if isinstance(image_source, str):
                # URL å›¾åƒ
                if image_source.startswith(('http://', 'https://')):
                    try:
                        import requests
                        response = requests.get(image_source, timeout=10)
                        response.raise_for_status()
                        image = Image.open(BytesIO(response.content)).convert('RGB')
                        logger.info(f"æˆåŠŸä» URL åŠ è½½å›¾åƒ: {image_source}")
                        return image
                    except Exception as e:
                        logger.error(f"URL å›¾åƒåŠ è½½å¤±è´¥ {image_source}: {e}")
                        return None
                
                # æœ¬åœ°æ–‡ä»¶è·¯å¾„
                elif os.path.isfile(image_source):
                    try:
                        image = Image.open(image_source).convert('RGB')
                        logger.info(f"æˆåŠŸä»æœ¬åœ°æ–‡ä»¶åŠ è½½å›¾åƒ: {image_source}")
                        return image
                    except Exception as e:
                        logger.error(f"æœ¬åœ°å›¾åƒæ–‡ä»¶åŠ è½½å¤±è´¥ {image_source}: {e}")
                        return None
                
                # Base64 å­—ç¬¦ä¸²
                else:
                    try:
                        # å¤„ç† data:image/xxx;base64,... æ ¼å¼
                        if image_source.startswith('data:image'):
                            base64_data = image_source.split(',')[1] if ',' in image_source else image_source
                        else:
                            base64_data = image_source
                        
                        # è§£ç  base64
                        img_data = base64.b64decode(base64_data)
                        image = Image.open(BytesIO(img_data)).convert('RGB')
                        logger.info("æˆåŠŸä» base64 å­—ç¬¦ä¸²åŠ è½½å›¾åƒ")
                        return image
                    except Exception as e:
                        logger.error(f"Base64 å›¾åƒè§£ç å¤±è´¥: {e}")
                        return None
            
            logger.warning(f"ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼: {type(image_source)}")
            return None
            
        except Exception as e:
            logger.error(f"å›¾åƒåŠ è½½è¿‡ç¨‹å‡ºé”™: {e}")
            return None
    
    @classmethod
    def from_env(cls, model_name: str, context: Dict[str, Any]) -> "UnifiedModelManager":
        """
        ä»ç¯å¢ƒé…ç½®åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            context: ä¸Šä¸‹æ–‡é…ç½®
        
        Returns:
            æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
        """
        # è·å–è®¾å¤‡é…ç½®
        device_config = context.get("device_config", {"gpu_ids": [0]})
        
        # è·å–ç¦»çº¿æ¨¡å¼è®¾ç½®
        offline_mode = context.get("offline_mode", False)
        
        return cls(model_name, device_config, offline_mode)
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if self.model:
            try:
                del self.model
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except Exception as e:
                logger.warning(f"æ¸…ç†æ¨¡å‹èµ„æºæ—¶å‡ºé”™: {e}")
    
    @staticmethod
    def list_supported_models() -> str:
        """
        è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„æ¨¡å‹åˆ—è¡¨å­—ç¬¦ä¸²
        """
        result = "æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ï¼š\n\n"
        result += "| æ¨¡å‹åç§° | ç±»å‹ | æè¿° | æœ€å°GPUå†…å­˜ |\n"
        result += "|----------|------|------|-------------|\n"
        
        for name, info in SUPPORTED_MODELS.items():
            result += f"| {name} | {info['type']} | {info['description']} | {info['min_gpu_memory']} |\n"
        
        return result
    
    def _extract_query_content(self, messages):
        """ä»æ¶ˆæ¯ä¸­æå–æŸ¥è¯¢å†…å®¹ï¼Œæ”¯æŒå¤æ‚æ ¼å¼"""
        try:
            if not messages:
                return ""
            
            last_message = messages[-1]
            content = last_message.get('content', '')
            
            # å¦‚æœcontentæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
            if isinstance(content, str):
                return content
            
            # å¦‚æœcontentæ˜¯åˆ—è¡¨ï¼ˆå®˜æ–¹æ ¼å¼ï¼‰ï¼Œæå–æ–‡æœ¬éƒ¨åˆ†
            if isinstance(content, list):
                text_parts = []
                for item in content:
                    if isinstance(item, dict) and item.get('type') == 'text':
                        text_parts.append(item.get('text', ''))
                return ' '.join(text_parts)
            
            return str(content)
        except Exception:
            return "" 