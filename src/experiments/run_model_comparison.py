#!/usr/bin/env python3
"""
OnCallAgent å¼€æºæ¨¡å‹å¯¹æ¯”å®éªŒ
æµ‹è¯•ä¸åŒå¼€æºLLM/VLMä½œä¸ºAgent backendçš„æ•ˆæœ
"""

import json
import time
import argparse
import asyncio
from pathlib import Path
from typing import Dict, List, Any
import sys
import os
import copy

# æ·»åŠ OnCallAgentåˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.oncall_agent.main import OnCallAgent
from model_configs import list_supported_models, get_model_config

class ModelComparisonExperiment:
    """å¼€æºæ¨¡å‹å¯¹æ¯”å®éªŒ"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.base_config = json.load(f)
    
    def create_model_config(self, model_name: str) -> Dict[str, Any]:
        """ä¸ºæŒ‡å®šæ¨¡å‹åˆ›å»ºé…ç½®"""
        config = copy.deepcopy(self.base_config)
        
        # æ›´æ–°æ‰€æœ‰Agentä½¿ç”¨æŒ‡å®šæ¨¡å‹
        for agent_name in config["agents"]:
            if "model_name" in config["agents"][agent_name]:
                config["agents"][agent_name]["model_name"] = model_name
        
        # è®¾ç½®é»˜è®¤æ¨¡å‹
        config["default_model"] = model_name
        
        # æ ¹æ®æ¨¡å‹è°ƒæ•´ç”Ÿæˆå‚æ•°
        model_config = get_model_config(model_name)
        for agent_name in config["agents"]:
            if "max_tokens" in config["agents"][agent_name]:
                config["agents"][agent_name]["max_tokens"] = model_config.generation_params.get("max_new_tokens", 512)
            if "temperature" in config["agents"][agent_name]:
                config["agents"][agent_name]["temperature"] = model_config.generation_params.get("temperature", 0.7)
        
        return config
    
    async def evaluate_query_with_model(self, query_data: Dict[str, Any], model_name: str) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹è¯„ä¼°æŸ¥è¯¢"""
        start_time = time.time()
        
        try:
            # åˆ›å»ºæ¨¡å‹ç‰¹å®šé…ç½®
            model_config = self.create_model_config(model_name)
            
            # åˆå§‹åŒ–OnCallAgent
            oncall_agent = OnCallAgent(model_config)
            
            # å‡†å¤‡æŸ¥è¯¢ä¸Šä¸‹æ–‡
            query_context = {
                "query": query_data["question"],
                "image": None,  # å®é™…å®ç°ä¸­éœ€è¦å¤„ç†å›¾åƒ
                "context": {"keywords": query_data.get("keywords", [])},
                "model": model_name,
                "type": "online"
            }
            
            # å¤„ç†æŸ¥è¯¢
            result = await oncall_agent.process_query(query_context)
            
            response = result.get("answer", "æ— å“åº”")
            confidence = result.get("confidence", 0.0)
            metadata = result.get("metadata", {})
            
        except Exception as e:
            response = f"å¤„ç†å¤±è´¥: {str(e)}"
            confidence = 0.0
            metadata = {"error": str(e)}
        
        return {
            "query_id": query_data.get("id", "unknown"),
            "model_name": model_name,
            "response": response,
            "response_time": time.time() - start_time,
            "confidence": confidence,
            "metadata": metadata,
            "ground_truth": query_data.get("answer", ""),
            "difficulty": query_data.get("difficulty", "medium"),
            "type": query_data.get("type", "text"),
            "source_doc": query_data.get("source_doc", "")
        }
    
    async def run_model_comparison(self, dataset_path: str, models: List[str], 
                                 batch_size: int = 5) -> Dict[str, List[Dict[str, Any]]]:
        """è¿è¡Œå¤šæ¨¡å‹å¯¹æ¯”å®éªŒ"""
        print(f"å¼€å§‹æ¨¡å‹å¯¹æ¯”å®éªŒï¼Œæµ‹è¯• {len(models)} ä¸ªæ¨¡å‹")
        
        # åŠ è½½æ•°æ®é›†
        with open(dataset_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        
        # ä¸ºäº†å¿«é€Ÿå®éªŒï¼Œåªä½¿ç”¨éƒ¨åˆ†æ•°æ®
        if len(dataset) > 100:
            dataset = dataset[:100]
        
        print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        all_results = {}
        
        for model_name in models:
            print(f"\næ­£åœ¨æµ‹è¯•æ¨¡å‹: {model_name}")
            model_results = []
            
            # æ‰¹é‡å¤„ç†
            for i in range(0, len(dataset), batch_size):
                batch = dataset[i:i + batch_size]
                print(f"  æ‰¹æ¬¡ {i // batch_size + 1}/{(len(dataset) + batch_size - 1) // batch_size}")
                
                # ä¸²è¡Œå¤„ç†ï¼ˆé¿å…GPUå†…å­˜ä¸è¶³ï¼‰
                for query_data in batch:
                    try:
                        result = await self.evaluate_query_with_model(query_data, model_name)
                        model_results.append(result)
                    except Exception as e:
                        print(f"  æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
                        continue
            
            all_results[model_name] = model_results
            print(f"âœ“ æ¨¡å‹ {model_name} å®Œæˆï¼Œå…± {len(model_results)} ä¸ªç»“æœ")
        
        return all_results

def calculate_model_metrics(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è®¡ç®—æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
    if not results:
        return {}
    
    total_queries = len(results)
    avg_response_time = sum(r["response_time"] for r in results) / total_queries
    avg_confidence = sum(r["confidence"] for r in results) / total_queries
    success_rate = len([r for r in results if r["confidence"] > 0.5]) / total_queries
    
    # GPUå†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    gpu_memory_info = {}
    if results[0]["metadata"].get("gpu_memory"):
        gpu_memory_info = {
            "peak_memory_mb": max(r["metadata"].get("gpu_memory", {}).get("peak", 0) for r in results),
            "avg_memory_mb": sum(r["metadata"].get("gpu_memory", {}).get("avg", 0) for r in results) / total_queries
        }
    
    # æŒ‰éš¾åº¦åˆ†æ
    difficulty_analysis = {}
    for difficulty in ["easy", "medium", "hard"]:
        difficulty_results = [r for r in results if r["difficulty"] == difficulty]
        if difficulty_results:
            difficulty_analysis[difficulty] = {
                "count": len(difficulty_results),
                "avg_confidence": sum(r["confidence"] for r in difficulty_results) / len(difficulty_results),
                "success_rate": len([r for r in difficulty_results if r["confidence"] > 0.5]) / len(difficulty_results)
            }
    
    return {
        "total_queries": total_queries,
        "avg_response_time": avg_response_time,
        "avg_confidence": avg_confidence,
        "success_rate": success_rate,
        "difficulty_analysis": difficulty_analysis,
        "gpu_memory_info": gpu_memory_info
    }

async def main():
    parser = argparse.ArgumentParser(description="è¿è¡ŒOnCallAgentæ¨¡å‹å¯¹æ¯”å®éªŒ")
    parser.add_argument("--dataset", required=True, help="æ•°æ®é›†æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", required=True, help="ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--config", required=True, help="åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--models", nargs='+', help="è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨", 
                       default=["Qwen/Qwen2.5-7B-Instruct", "THUDM/chatglm3-6b"])
    parser.add_argument("--batch_size", type=int, default=5, help="æ‰¹å¤„ç†å¤§å°")
    
    args = parser.parse_args()
    
    # éªŒè¯æ¨¡å‹æ˜¯å¦æ”¯æŒ
    supported_models = list_supported_models()
    valid_models = [m for m in args.models if m in supported_models]
    
    if not valid_models:
        print(f"é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ¨¡å‹ã€‚æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨:")
        for model in supported_models:
            print(f"  - {model}")
        return
    
    print(f"å°†æµ‹è¯•ä»¥ä¸‹æ¨¡å‹: {valid_models}")
    
    # è¿è¡Œæ¨¡å‹å¯¹æ¯”å®éªŒ
    experiment = ModelComparisonExperiment(args.config)
    all_results = await experiment.run_model_comparison(
        args.dataset, valid_models, args.batch_size
    )
    
    # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„æŒ‡æ ‡
    model_metrics = {}
    for model_name, results in all_results.items():
        model_metrics[model_name] = calculate_model_metrics(results)
    
    # ä¿å­˜ç»“æœ
    output_data = {
        "experiment_type": "model_comparison",
        "tested_models": valid_models,
        "model_metrics": model_metrics,
        "detailed_results": all_results,
        "experiment_time": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ æ¨¡å‹å¯¹æ¯”å®éªŒç»“æœå·²ä¿å­˜åˆ° {args.output}")
    
    # è¾“å‡ºæ€§èƒ½æ’å
    print("\nğŸ“Š æ¨¡å‹æ€§èƒ½æ’å:")
    model_ranking = sorted(model_metrics.items(), 
                          key=lambda x: x[1].get("avg_confidence", 0), reverse=True)
    
    for i, (model_name, metrics) in enumerate(model_ranking, 1):
        print(f"  {i}. {model_name}")
        print(f"     å‡†ç¡®ç‡: {metrics['avg_confidence']:.3f}")
        print(f"     å“åº”æ—¶é—´: {metrics['avg_response_time']:.2f}s")
        print(f"     æˆåŠŸç‡: {metrics['success_rate']:.1%}")

if __name__ == "__main__":
    asyncio.run(main())
