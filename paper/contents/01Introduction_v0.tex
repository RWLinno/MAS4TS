\section{Introduction}

Time series forecasting is critical for numerous real-world applications, including financial analysis \cite{idrees2019prediction}, weather prediction \cite{karevan2020transductive}, energy estimation \cite{deb2017review}, and traffic management \cite{zheng2020traffic}. Traditional deep learning approaches, ranging from LSTMs to advanced Transformer-based models like LogTrans \cite{li2019logtrans}, Informer \cite{zhou2021informer}, and PatchTST \cite{nie2022time}, have primarily focused on modeling temporal dependencies using historical time series data, sometimes combined with frequency domain analysis. While these methods have achieved significant progress, they often struggle with generalization across diverse domains and scenarios with limited training data, particularly in few-shot and zero-shot settings.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/introduction.png}
    \caption{Comparison of LLM-enhanced, Vision-enhanced, and our \method for time series forecasting.}
    \label{fig:intro}
    \vspace{-2em}
\end{figure}

Recent work has explored time series forecasting with additional modalities, primarily falling into two categories:

\textbf{LLM-enhanced Models:} Approaches like TimeLLM \cite{jin2023time} and UniTime \cite{liu2024unitime} leverage large language models by projecting time series into textual representations (Figure~\ref{fig:intro}, upper right). While this paradigm benefits from LLMs' extensive pretrained knowledge and remarkable few-shot/zero-shot capabilities, it suffers from a fundamental limitation: the transformation of continuous temporal patterns into discrete textual tokens inevitably leads to information loss, particularly for fine-grained temporal features. Moreover, pretrained word embeddings are inherently ill-suited for capturing subtle temporal dynamics.

\textbf{Vision-enhanced Models:} In parallel, vision-based approaches \cite{wu2023timesnet, wang2024timemixer++} show that time series naturally align with visual representations. By converting time series into image-like inputs, these methods can leverage pretrained vision models to effectively capture hierarchical temporal structures (Figure~\ref{fig:intro}, upper left). Notably, VisionTS \cite{chen2024visiontsvisualmaskedautoencoders} achieves competitive zero-shot forecasting performance, highlighting the potential of visual representations. However, these methods often lack semantic understanding and contextual reasoning capabilities that are crucial for complex forecasting scenarios.

Given the complementary strengths and limitations of LLM and vision-enhanced models, an ideal solution should leverage textual, visual, and temporal information simultaneously. In real-world applications, multimodal time series naturally exist--such as medical ECG signals with records (text) and CT scans (images)--but effectively utilizing these modalities remains challenging due to their structural differences: time series are sequential, text relies on discrete tokens, and images are spatially structured. Existing approaches often focus on single-modality augmentation, failing to fully exploit the potential of multimodal representations.

Motivated by these insights, we propose \method, a unified framework that utilizes pretrained Vision-Language Models (VLMs) to bridge vision, language, and temporal modalities for enhanced time series forecasting (Figure~\ref{fig:intro}, bottom). VLMs offer unique advantages over LLMs and vision models: (1) their pretrained vision-language alignment enables seamless multimodal integration; (2) they provide a vision-language space for time series projection, bridging modality gaps and avoiding single-modality limitations; and (3) they combine LLMs' semantic reasoning with vision models' temporal pattern recognition. Specifically, \method introduces three key components: (1) the \textbf{Vision-Augmented Learner}, which adaptively transforms time series into images using multi-scale convolution, frequency and periodic encoding, preserving both fine-grained details and high-level structures; (2) the \textbf{Text-Augmented Learner}, which generates rich textual prompts (e.g., statistics, trends, and periodicity) to complement the visual representations; and (3) the \textbf{Retrieval-Augmented Learner}, which processes raw time series data through patch-based feature extraction and memory bank interactions to generate enriched temporal representations. These modules collaborate with VLMs to combine visual and textual multimodal representations with temporal memory bank features, producing accurate forecasts through a fine-tuned prediction head.

Our key contributions are summarized as follows:

\vspace{-1em}
\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item We present the first framework leveraging pretrained VLMs for time series forecasting, combining visual, textual, and temporal modalities in a unified architecture.

    \item We introduce three key components: a Vision-Augmented Learner for adaptive time series-to-image transformation, a Text-Augmented Learner for contextual prompt generation, and a Retrieval-Augmented Learner for temporal feature enhancement through memory bank interactions.

    \item \method achieves state-of-the-art performance in both few-shot and zero-shot scenarios, establishing a new direction for multimodal time series forecasting.
\end{itemize}