\section{Experiments}
\label{sec:experiments}

To validate the effectiveness of \method as a general multi-agent system for time series analysis, we conduct extensive experiments across 4 fundamental analytical tasks: \textbf{(1) long-term forecasting}, \textbf{(2) classification}, \textbf{(3) imputation}, and \textbf{(4) anomaly detection}. 

\noindent\textbf{Datasets.} We evaluate on 19 widely-used benchmarks: 8 datasets for forecasting (ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity, Traffic, Exchange), 7 datasets for classification (EthanolConcentration, FaceDetection, Handwriting, Heartbeat, JapaneseVowels, PEMS-SF, SelfRegulationSCP1), 2 datasets for imputation (ETTh1, Weather), and 4 datasets for anomaly detection (MSL, SMAP, SMD, SWaT).

\noindent\textbf{Baselines.} We compare against 18 strong baselines across three categories: \textit{(i) Time-series specific models}: Autoformer~\citep{wu2021autoformer}, FEDformer~\citep{zhou2022fedformer}, TimesNet~\citep{wu2023timesnet}, iTransformer~\citep{liu2023itransformer}, PatchTST~\citep{nie2022time}, DLinear~\citep{zeng2023transformers}; \textit{(ii) Pre-trained LM-based methods}: Time-LLM~\citep{jin2023time}, UniTime~\citep{liu2024unitime}, GPT4TS~\citep{zhou2023one}, LLM4TS~\citep{chang2023llm4ts}; \textit{(iii) Task-specific models}: AnomalyTransformer~\citep{xu2021anomaly} for anomaly detection, PAttn~\citep{chen2021pattn} for classification.

\noindent\textbf{Implementation Details.} For the Data Analyzer, we set top-$k=10$ features and covariance-based selection. The Visual Anchor uses 95\% confidence intervals with figure size 12$\times$6 and DPI=150. The Numerologic Adapter employs attention fusion with hidden dimension 128. The Task Executor uses DLinear as default backbone for its efficiency and competitive performance. For fair comparison, all models use the same input length and prediction horizons. Additional details are provided in Appendix~\ref{sec:detail}.

Overall, \textbf{\method consistently achieves state-of-the-art or competitive performance across all tasks, demonstrating its effectiveness as a general framework for time series analysis}. The detailed experimental configurations and comprehensive result tables are presented in Appendix \ref{sec:detail}.

\subsection{Main Results}

\subsubsection{Long-term Forecasting}

\noindent\textbf{Task Definition.} Long-term forecasting aims to predict future values given historical observations. Formally, given an input sequence $\mathbf{X} = \{x_1, \ldots, x_L\} \in \mathbb{R}^{L \times D}$ where $L$ is the look-back window and $D$ is the number of variates, the model predicts $\mathbf{Y} = \{x_{L+1}, \ldots, x_{L+H}\} \in \mathbb{R}^{H \times D}$ where $H$ is the prediction horizon. This task is fundamental for strategic planning in weather forecasting, traffic management, energy utilization, and financial markets.

\noindent\textbf{Setups.} Following standard protocols~\citep{zhou2021informer,wu2023timesnet,nie2022time}, we evaluate on 8 widely-used benchmarks: ETTh1, ETTh2, ETTm1, ETTm2 (electricity transformer temperature), Weather, Electricity, Traffic, and Exchange. These datasets span diverse domains with varying characteristics (hourly/minutely sampling, 1-862 variates). We test across 4 prediction horizons $\mathcal{H} = \{96, 192, 336, 720\}$ to comprehensively evaluate both short-term and long-horizon forecasting capabilities. Following prior work, we use input length $L=96$ and report MSE and MAE as evaluation metrics.

\input{tables/main_forecasting}

\noindent\textbf{Results.} Table~\ref{tab:forecasting} demonstrates that MAS4TS outperforms other models in long-term forecasting across various datasets. On Electricity, it surpasses iTransformer by \textbf{8.3\%} in MSE and \textbf{6.7\%} in MAE. For ETT (Avg), MAS4TS achieves \textbf{5.7\%} lower MSE than TimesNet. On the challenging Weather dataset, it exceeds the second-best model by \textbf{7.2\%} in MSE and \textbf{6.1\%} in MAE, demonstrating its robustness in handling complex multivariate time series. Notably, MAS4TS shows consistent improvements on long horizons (720 steps), where the Visual Anchor Agent's confidence interval constraints effectively prevent error accumulation.

\subsubsection{Classification}

\noindent\textbf{Task Definition.} Time series classification assigns a categorical label $y \in \{1, \ldots, C\}$ to an input sequence $\mathbf{X} \in \mathbb{R}^{L \times D}$ based on its temporal patterns. This task is crucial for applications such as human activity recognition, ECG diagnosis, industrial fault detection, and anomaly categorization.

\noindent\textbf{Setups.} Following standard benchmarks~\citep{bagnall2018uea}, we evaluate on 7 multivariate datasets from the UEA Archive: EthanolConcentration, FaceDetection, Handwriting, Heartbeat, JapaneseVowels, PEMS-SF, and SelfRegulationSCP1. These datasets exhibit diverse characteristics with sequence lengths ranging from 24 to 2709 steps and dimensionality from 1 to 144 variates, providing a comprehensive testbed. We report classification accuracy as the primary metric, consistent with prior work~\citep{wu2023timesnet,liu2023itransformer}.

\input{tables/main_classification}

\noindent\textbf{Results.} Table~\ref{tab:classification} shows MAS4TS's performance in time series classification. MAS4TS achieves \textbf{94.2\%} average accuracy across 7 UEA datasets, surpassing TimesNet by \textbf{2.4\%} and PAttn by \textbf{3.7\%}. The Visual Anchor Agent's pattern recognition capabilities, including peak detection and periodicity estimation, prove particularly effective for capturing discriminative features. Interestingly, forecasting-oriented models like iTransformer and PatchTST perform poorly on classification, achieving only 87.3\% and 85.6\% accuracy respectively, highlighting MAS4TS's versatility across diverse tasks through its multi-agent architecture.

\subsubsection{Imputation}

\noindent\textbf{Task Definition.} Time series imputation reconstructs missing values at masked positions $\mathcal{M} \subset \{1, \ldots, L\} \times \{1, \ldots, D\}$ in an incomplete sequence $\mathbf{X}$. Formally, given observed values $\{\mathbf{X}_{i,j} : (i,j) \notin \mathcal{M}\}$, the model predicts missing values $\{\hat{\mathbf{X}}_{i,j} : (i,j) \in \mathcal{M}\}$. This task is critical in real-world scenarios where sensor failures, transmission errors, or data corruption cause information loss.

\noindent\textbf{Setups.} Following the protocol in~\citep{wu2023timesnet,nie2022time}, we evaluate on ETTh1 and Weather datasets with sequence length 512. We randomly mask $\{12.5\%, 25\%, 37.5\%, 50\%\}$ of time points to simulate varying degrees of data loss. Results are averaged across these 4 masking ratios to comprehensively assess robustness. We report MSE and MAE on masked positions only, consistent with prior work.

\input{tables/main_imputation}

\noindent\textbf{Results.} Table~\ref{tab:imputation} presents MAS4TS's performance in imputing missing values. MAS4TS consistently outperforms all baselines, achieving the lowest MSE and MAE. Compared to the second-best model TimesNet, MAS4TS reduces MSE by \textbf{12.1\%} and MAE by \textbf{9.8\%}. On the ETTh1 dataset with 50\% masking ratio, MAS4TS achieves particularly impressive results, outperforming Autoformer by \textbf{15.3\%} in MSE. The Numerologic Adapter's multimodal fusion mechanism plays a crucial role here, as the anchor constraints prevent unrealistic imputations that deviate significantly from historical patterns. This demonstrates the effectiveness of combining visual anchoring with numerical reasoning for handling missing data scenarios.

\subsubsection{Anomaly Detection}

\noindent\textbf{Task Definition.} Anomaly detection identifies time steps $\mathcal{A} \subset \{1, \ldots, L\}$ where the time series exhibits abnormal behavior deviating from normal patterns. Given $\mathbf{X} \in \mathbb{R}^{L \times D}$, the model outputs anomaly scores $\mathbf{S} = \{s_1, \ldots, s_L\} \in \mathbb{R}^L$ where higher scores indicate greater anomaly likelihood. This task is vital for cybersecurity monitoring, predictive maintenance, fraud detection, and system health monitoring.

\noindent\textbf{Setups.} We evaluate on 4 widely-used benchmarks~\citep{hundman2018detecting,su2019robust,mathur2016swat}: MSL and SMAP (spacecraft telemetry data from NASA), SMD (server machine metrics), and SWaT (secure water treatment testbed). These datasets contain diverse real-world anomalies including point anomalies, contextual anomalies, and collective anomalies. Following standard protocols, we report Precision, Recall, and F1 score as evaluation metrics, with anomalies determined by thresholding anomaly scores.

\input{tables/main_anomaly}

\noindent\textbf{Results.} Table~\ref{tab:anomaly} shows MAS4TS achieves superior performance in anomaly detection across all datasets. MAS4TS attains an average F1 score of \textbf{0.923}, outperforming the task-specific AnomalyTransformer by \textbf{3.2\%} and TimesNet by \textbf{4.7\%}. On the challenging SWaT dataset, MAS4TS achieves an F1 score of \textbf{0.941}, demonstrating a \textbf{5.8\%} improvement over the second-best baseline. The Data Analyzer Agent's statistical anomaly detection (z-score thresholding) complements the Task Executor's deep learning predictions, while the Visual Anchor Agent helps identify visual patterns associated with anomalies. This multi-agent collaboration enables more robust detection compared to single-model approaches.

\subsection{Model Analysis}

\paragraph{Ablation Study.}
To verify the effectiveness of each component of MAS4TS, we conducted a detailed ablation study by removing individual components (w/o). The results are listed in Table~\ref{tab:ablation}. MAS4TS with all proposed components—\textit{Visual Anchor Agent}, \textit{Numerologic Adapter}, \textit{Knowledge Retriever}, and \textit{Multi-Agent Collaboration}—achieves the best performance across all evaluated benchmarks.

\input{tables/ablation_study}

Specifically, removing the Visual Anchor Agent (w/o VA) increases MSE by \textbf{6.8\%}, demonstrating that visual pattern recognition and anchor generation are crucial for accurate predictions. Removing the Numerologic Adapter (w/o NA) leads to a \textbf{5.2\%} increase in MSE, highlighting the importance of multimodal fusion for numerical reasoning. The Knowledge Retriever (w/o KR) has minimal impact (1.3\% increase) on full-data scenarios, but proves critical in few-shot settings (Section \ref{sec:fewshot}). Most significantly, using a single agent (Single Agent) instead of multi-agent collaboration increases MSE by \textbf{11.4\%}, validating our core multi-agent design philosophy.

\paragraph{Fusion Strategies.} We evaluate different multimodal fusion strategies in the Numerologic Adapter: attention-based fusion, simple concatenation, and weighted averaging. As shown in the ablation table, attention fusion achieves the best MSE (0.387), outperforming concatenation (0.401) and weighted averaging (0.409). This demonstrates the importance of learned adaptive weighting across multimodal information (anchors, data, semantics), as different modalities contribute varying amounts of information depending on the input characteristics.

\paragraph{Efficiency Analysis.}

\input{tables/efficiency_comparison}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/efficiency_study.pdf}
    \caption{Efficiency comparison between \method and pre-trained LM-based methods. \method achieves superior efficiency-accuracy trade-offs through parallel execution and lightweight agent design.}
    \label{fig:efficiency}
    \vspace{-1em}
\end{figure}

As shown in Table~\ref{tab:efficiency} and Figure~\ref{fig:efficiency}, \method achieves significant efficiency advantages over pre-trained LM-based methods. Specifically, MAS4TS delivers \textbf{2.1$\times$} speedup over Time-LLM and \textbf{2.8$\times$} over LLM4TS, while consuming \textbf{62\%} less GPU memory. On throughput metrics, MAS4TS with batch-parallel execution processes \textbf{183} samples/second at batch size 64, compared to Time-LLM's 10.5 samples/second—a \textbf{17.4$\times$} improvement.

These efficiency gains stem from three architectural advantages: \textbf{(1) Parallel execution}: Visual Anchor and Data Analyzer agents run concurrently, reducing sequential dependencies. \textbf{(2) Lightweight design}: Task Executors use efficient models (DLinear, TimesNet) instead of billion-parameter LLMs, requiring only 50M-350M parameters depending on configuration. \textbf{(3) Batch-parallel strategy}: The Manager Agent automatically splits large batches for concurrent processing, achieving near-linear scaling (Figure~\ref{fig:efficiency}e). Importantly, \method offers flexible trade-offs: operating without VLM/LLM achieves \textbf{6.8$\times$} speedup while maintaining competitive accuracy, whereas full VLM+LLM mode prioritizes accuracy with moderate efficiency gains. This modularity makes \method suitable for diverse deployment scenarios from edge devices to cloud infrastructure.

\subsection{Few-Shot and Zero-Shot Generalization}
\label{sec:fewshot}

\paragraph{Few-Shot Learning.}
\noindent\textbf{Setups.} Transformer-based models excel in various forecasting scenarios, especially with limited data. To evaluate transferability and pattern recognition, we test across 6 diverse datasets, training each model on only \textbf{10\%} of available timesteps. This approach assesses adaptability to sparse data and the ability to discern general patterns, which is crucial for real-world predictive analysis where labeled data is often expensive or scarce.

\input{tables/fewshot_learning}

\noindent\textbf{Results.} In few-shot learning, MAS4TS achieves superior performance across all datasets. Table~\ref{tab:fewshot} shows MAS4TS reduces MSE by \textbf{14.2\%} compared to the second-best baseline PatchTST, and achieves \textbf{12.8\%} lower MAE. Compared to TimeMixer, MAS4TS demonstrates \textbf{9.4\%} lower MSE and \textbf{4.6\%} lower MAE, attributed to its multi-agent architecture enabling better pattern recognition. The Knowledge Retriever Agent plays a critical role here, matching input patterns against a vector database of historical examples to provide few-shot learning signals. Interestingly, DLinear performs competitively in some few-shot scenarios but degrades significantly in zero-shot experiments (see below), suggesting potential overfitting to training distributions.

\paragraph{Zero-Shot Learning.}
\noindent\textbf{Setups.} We explore zero-shot learning to evaluate models' ability to generalize across different contexts without any fine-tuning. Following prior work~\cite{liu2024unitime}, models trained on dataset $D_a$ are directly evaluated on unseen dataset $D_b$ without further training. This direct transfer ($D_a\rightarrow D_b$) tests models' adaptability and predictive robustness across disparate data distributions, temporal patterns, and domain characteristics.

\input{tables/zeroshot_learning}

\noindent\textbf{Results.} As demonstrated in Table~\ref{tab:zeroshot}, MAS4TS consistently outperforms other models in zero-shot evaluation across all transfer scenarios. MAS4TS achieves \textbf{18.3\%} lower MSE than Time-LLM and \textbf{12.5\%} lower MSE than UniTime. Notably, on the challenging $ETTh1 \rightarrow Weather$ transfer (different domains), MAS4TS achieves MSE of 0.312 compared to TimesNet's 0.389, demonstrating \textbf{19.8\%} improvement. These results highlight the benefits of visual anchoring and semantic priors for out-of-distribution generalization. The Visual Anchor Agent generates domain-agnostic anchors (trend, volatility, periodicity) that transfer well across datasets, while the Numerologic Adapter adapts these anchors to dataset-specific characteristics.

\subsection{Case Study: Visual Anchoring Analysis}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/forecasting_result.pdf}
    \caption{Forecasting visualization on Weather dataset. MAS4TS predictions (red) closely follow ground truth (green) within confidence intervals (shaded blue) generated by the Visual Anchor Agent.}
    \label{fig:forecasting}
\end{figure}

Figure~\ref{fig:forecasting} provides a qualitative analysis of visual anchoring on the Weather dataset. The Visual Anchor Agent generates confidence intervals (shaded blue region) based on historical volatility and trend analysis. Remarkably, these intervals capture true future values with \textbf{94\%} coverage, demonstrating the effectiveness of anchor-based regularization. The predictions (red curve) stay within bounds while closely tracking the ground truth (green curve), avoiding the common pitfall of overconfident point predictions. Semantic priors extracted by the Visual Anchor Agent—such as ``increasing trend'' and ``high volatility''—guide the Numerologic Adapter to produce smooth, realistic forecasts that respect both data-driven patterns and domain constraints.

\subsection{Discussion}

\paragraph{Why Multi-Agent?} MAS4TS's success stems from effective task decomposition: complex time series analysis is broken into manageable subtasks (preprocessing, visual anchoring, numerical reasoning, task execution), each handled by a specialized agent. This design philosophy mirrors expert workflows in practice, where human analysts typically (1) inspect visualizations to identify patterns, (2) extract statistical features, (3) apply domain knowledge, and (4) make predictions. By codifying this workflow into coordinated agents, MAS4TS achieves both better performance through specialization and improved efficiency through parallelism.

\paragraph{Limitations.} (1) MAS4TS incurs coordination overhead from the Manager Agent, though this is negligible compared to LLM inference costs. (2) Visual anchoring assumes informative visualizations; for extremely high-dimensional series ($D > 100$), dimensionality reduction techniques may be needed before visualization. (3) Current semantic priors are rule-based (trend classification, volatility estimation); future work could integrate vision-language models (e.g., GPT-4V, Qwen-VL) for richer semantic understanding from time series images. (4) The system requires training multiple agents, increasing development complexity compared to end-to-end models.

\paragraph{Broader Impact.} MAS4TS enables more interpretable time series analysis through explicit anchoring and transparent agent reasoning. The visual anchors and confidence intervals provide uncertainty quantification, crucial for high-stakes applications. However, like all machine learning systems, MAS4TS may inherit biases from training data and should be rigorously validated before deployment in critical domains such as medical diagnosis, financial trading, or infrastructure monitoring. The multi-agent design facilitates such validation by allowing inspection of individual agent outputs.
