\section{Methodology}
\label{sec:method}

\subsection{Problem Formulation and Overview}

\noindent\textbf{Problem Definition.} Given a multivariate time series $\mathbf{X} = \{x_1, x_2, \ldots, x_L\} \in \mathbb{R}^{L \times D}$ with $L$ time steps and $D$ features, we consider four fundamental tasks: \textbf{(1) Forecasting}: predict future values $\mathbf{Y} = \{x_{L+1}, \ldots, x_{L+H}\} \in \mathbb{R}^{H \times D}$ where $H$ is the prediction horizon; \textbf{(2) Classification}: assign categorical label $y \in \{1, \ldots, C\}$ where $C$ is the number of classes; \textbf{(3) Imputation}: reconstruct missing values $\{\hat{x}_{i,j} : (i,j) \in \mathcal{M}\}$ at masked positions $\mathcal{M} \subset \{1, \ldots, L\} \times \{1, \ldots, D\}$; \textbf{(4) Anomaly Detection}: identify anomalous time steps $\mathcal{A} \subset \{1, \ldots, L\}$ based on deviation from normal patterns.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/framework.pdf}
    \caption{Architecture of \method showing the multi-agent collaborative workflow. The Manager Agent orchestrates four specialized agents: Data Analyzer extracts features and selects top-$k$ variables, Visual Anchor generates prediction anchors from time series images, Numerologic Adapter fuses multimodal information for numerical reasoning, and Task Executor produces task-specific predictions with anchor constraints.}
    \label{fig:framework}
    \vspace{-1em}
\end{figure*}

\noindent\textbf{Framework Overview.} As shown in Figure~\ref{fig:framework}, \method adopts a \emph{plan-and-execute} multi-agent architecture~\citep{huang2024understanding} where a Manager Agent dynamically decomposes tasks into specialized stages executed by four core agents. Unlike rigid static planning~\citep{qian2024chatdev}, our Manager adaptively creates execution plans based on task type and enables parallel agent execution for efficiency. For forecasting tasks, the workflow consists of four stages: \textbf{Stage 1} (Data Analysis): extract features $\mathbf{F}_{\text{stat}}$ and select top-$k$ variables; \textbf{Stage 2} (Visual Anchoring): generate prediction anchors $\mathcal{A}$ from time series visualizations; \textbf{Stage 3} (Numerical Reasoning): fuse multimodal information to produce refined representations $\mathbf{F}_{\text{fused}}$; \textbf{Stage 4} (Task Execution): invoke specialized models to generate final predictions $\hat{\mathbf{Y}}$ constrained by anchors. We detail each component below, explaining both the technical mechanisms and their design motivations.

\subsection{Visual Anchoring}
\label{sec:visual_anchor}

\noindent\textbf{Motivation.} Human experts often analyze time series through visual inspection, identifying trends, patterns, and anomalies from plots. However, existing methods either ignore this visual modality entirely (time-series specific models) or inefficiently encode it through text descriptions (LM-based methods). We propose to directly leverage the visual understanding capabilities of Vision-Language Models (VLMs) by converting time series into images and extracting \emph{prediction anchors}—semantically-grounded numerical constraints that guide downstream reasoning.

\noindent\textbf{Time Series to Image Conversion.} Given input $\mathbf{X} \in \mathbb{R}^{L \times D}$, we generate visualizations $\mathcal{I} = \{I_1, \ldots, I_D\}$ where each $I_d$ is a line plot of feature $d$ across time. Formally, we construct image $I_d$ by mapping time series values to pixel coordinates:
\begin{equation}
I_d(p, q) = \text{Render}\big(\{(t, x_{t,d})\}_{t=1}^L\big), \quad p \in [0, W], q \in [0, H]
\end{equation}
where $W \times H$ is the image resolution (default: 1200$\times$600 pixels at 150 DPI). Importantly, we \emph{remove all text annotations} (axis labels, titles) to create pure visual representations that VLMs can analyze without textual bias.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/visual_anchoring.pdf}
    \caption{Visual anchoring process: (a) historical time series, (b) text-free visualization for VLM analysis, (c) generated anchors with confidence intervals, (d) prediction comparison showing anchor effectiveness.}
    \label{fig:anchor}
    \vspace{-1em}
\end{figure}

\noindent\textbf{Anchor Generation via VLM.} We employ Qwen3-VL-235B-A22B-Instruct-BF16~\citep{qwen2vl} as our default VLM to analyze time series images and generate prediction anchors. Given image $I_d$ and statistical context $\mathbf{s} \in \mathbb{R}^K$ (mean, std, trend slope), we query the VLM with a structured prompt $\mathcal{P}(\mathbf{s})$ (detailed in Appendix~\ref{sec:prompts}):
\begin{equation}
\mathbf{r}_{\text{VLM}} = \text{VLM}(I_d, \mathcal{P}(\mathbf{s}); \theta_{\text{VLM}})
\end{equation}
where $\mathbf{r}_{\text{VLM}}$ contains: (1) visual observations (trend strength, periodicity, volatility), (2) prediction anchors at key time points, and (3) confidence intervals. We parse this response to obtain structured anchors $\mathcal{A} = \{\mathbf{a}_1, \ldots, \mathbf{a}_K\}$ where $K=5$ anchors are uniformly sampled across the prediction horizon $H$.

\noindent\textbf{Anchor Formulation.} Each anchor $\mathbf{a}_h$ for forecasting consists of:
\begin{equation}
\mathbf{a}_h = (\boldsymbol{\mu}_h, \boldsymbol{\sigma}_h, \mathbf{u}_h, \mathbf{l}_h) \in \mathbb{R}^{4D}
\end{equation}
where $\boldsymbol{\mu}_h \in \mathbb{R}^D$ is the point forecast, $\boldsymbol{\sigma}_h \in \mathbb{R}^D$ quantifies uncertainty, and $\mathbf{u}_h, \mathbf{l}_h \in \mathbb{R}^D$ define the $(1-\alpha)$ confidence interval. When VLM is unavailable, we use rule-based generation:
\begin{equation}
\label{eq:anchor_gen}
\begin{aligned}
\text{slope}_d &= \frac{\sum_{t=1}^L (t - \bar{t})(x_{t,d} - \bar{x}_d)}{\sum_{t=1}^L (t - \bar{t})^2}, \\
\mu_{h,d} &= x_{L,d} + h \cdot \text{slope}_d, \\
\sigma_{h,d} &= \text{std}(\{x_{t,d}\}_{t=1}^L) \cdot \sqrt{h/H}, \\
\mathbf{u}_{h,d} &= \mu_{h,d} + z_{\alpha/2} \sigma_{h,d}, \quad \mathbf{l}_{h,d} = \mu_{h,d} - z_{\alpha/2} \sigma_{h,d}
\end{aligned}
\end{equation}
where $\bar{t} = (L+1)/2$, $\bar{x}_d = \frac{1}{L}\sum_{t=1}^L x_{t,d}$, and $z_{\alpha/2}$ is the critical value for confidence level $\alpha$ (e.g., $z_{0.025} = 1.96$ for 95\% CI). The time-dependent uncertainty $\sqrt{h/H}$ models increasing prediction uncertainty over longer horizons, consistent with statistical forecasting theory~\citep{hyndman2018forecasting}.

\noindent\textbf{Semantic Priors.} Beyond numerical anchors, we extract categorical semantic priors $\mathbf{p}_{\text{sem}} = (p_{\text{trend}}, p_{\text{vol}}, p_{\text{period}}) \in \{-1, 0, 1\}^3$ encoding trend direction, volatility level, and periodicity strength:
\begin{equation}
p_{\text{trend}} = \begin{cases}
+1 & \text{if } \text{slope} > \tau_{\uparrow} \text{ (increasing)} \\
-1 & \text{if } \text{slope} < -\tau_{\downarrow} \text{ (decreasing)} \\
0 & \text{otherwise (stable)}
\end{cases}
\end{equation}
with thresholds $\tau_{\uparrow}, \tau_{\downarrow} > 0$. Similarly, $p_{\text{vol}}$ and $p_{\text{period}}$ are determined from coefficient of variation and lag-1 autocorrelation. These discrete priors complement continuous anchors for robust multimodal reasoning.

\subsection{Numerical Reasoning}
\label{sec:numerical_reasoning}

\noindent\textbf{Motivation.} While visual anchors provide semantically-grounded constraints, directly applying them to predictions risks ignoring fine-grained numerical dynamics in the original time series. We address this through a \emph{Numerologic Adapter Agent} that performs explicit numerical reasoning by fusing multimodal information—visual anchors, statistical features, and semantic priors—to produce refined numerical constraints. This design enables the model to leverage pre-trained knowledge from VLMs/LLMs for semantic understanding while maintaining numerical precision through dedicated fusion mechanisms.

\noindent\textbf{Multimodal Feature Encoding.} Given input $\mathbf{X} \in \mathbb{R}^{L \times D}$, visual anchors $\mathcal{A}$, and semantic priors $\mathbf{p}_{\text{sem}}$, we encode three complementary representations:

\textbf{(1) Data Features} $\mathbf{f}_{\text{data}} \in \mathbb{R}^{B \times d}$ capture raw temporal statistics:
\begin{equation}
\mathbf{f}_{\text{data}} = \text{MLP}_{\text{data}}\big([\bar{\mathbf{x}}, \boldsymbol{\sigma}_{\mathbf{x}}, \min(\mathbf{X}), \max(\mathbf{X}), \mathbf{x}_{L-4:L}]\big)
\end{equation}
where $\bar{\mathbf{x}} = \frac{1}{L}\sum_{t=1}^L \mathbf{x}_t$ is the mean, $\boldsymbol{\sigma}_{\mathbf{x}}$ is the standard deviation, and $\mathbf{x}_{L-4:L}$ denotes the last 5 time steps for capturing recent dynamics.

\textbf{(2) Anchor Features} $\mathbf{f}_{\text{anchor}} \in \mathbb{R}^{B \times d}$ encode prediction constraints from visual anchoring:
\begin{equation}
\mathbf{f}_{\text{anchor}} = \text{MLP}_{\text{anchor}}\big([\bar{\boldsymbol{\mu}}, \bar{\boldsymbol{\sigma}}, \|\mathbf{u} - \mathbf{l}\|, p_{\text{trend}}]\big)
\end{equation}
where $\bar{\boldsymbol{\mu}} = \frac{1}{K}\sum_{k=1}^K \boldsymbol{\mu}_k$ averages anchor point forecasts, $\bar{\boldsymbol{\sigma}}$ aggregates uncertainties, $\|\mathbf{u} - \mathbf{l}\|$ measures interval width, and $p_{\text{trend}} \in \{-1,0,1\}$ encodes trend direction.

\textbf{(3) Semantic Features} $\mathbf{f}_{\text{semantic}} \in \mathbb{R}^{B \times d}$ embed categorical priors:
\begin{equation}
\mathbf{f}_{\text{semantic}} = \text{MLP}_{\text{sem}}([p_{\text{trend}}, p_{\text{vol}}, p_{\text{period}}])
\end{equation}

\noindent\textbf{Attention-Based Fusion.} We fuse the three modalities through learned attention weights, allowing the model to adaptively emphasize different information sources based on input characteristics:
\begin{equation}
\label{eq:fusion}
\begin{aligned}
\mathbf{a}_{\text{data}} &= \text{MLP}_{\text{attn}}(\mathbf{f}_{\text{data}}), \quad \mathbf{a}_{\text{anchor}} = \text{MLP}_{\text{attn}}(\mathbf{f}_{\text{anchor}}), \quad \mathbf{a}_{\text{sem}} = \text{MLP}_{\text{attn}}(\mathbf{f}_{\text{sem}}) \\
\boldsymbol{\alpha} &= \text{Softmax}([\mathbf{a}_{\text{data}}, \mathbf{a}_{\text{anchor}}, \mathbf{a}_{\text{sem}}]) \in \mathbb{R}^{B \times 3} \\
\mathbf{F}_{\text{fused}} &= \alpha_1 \odot \mathbf{f}_{\text{data}} + \alpha_2 \odot \mathbf{f}_{\text{anchor}} + \alpha_3 \odot \mathbf{f}_{\text{sem}}
\end{aligned}
\end{equation}
where $\odot$ denotes element-wise multiplication with broadcasting. This attention mechanism learns to weight modalities—for instance, relying more on visual anchors when trends are clear, or on raw data features when patterns are complex.

\noindent\textbf{LLM-Enhanced Reasoning (Optional).} For complex scenarios, we optionally enhance numerical reasoning through LLM ensemble. Given statistical context $\mathbf{s}$, visual analysis $\mathbf{r}_{\text{VLM}}$, and anchors $\mathcal{A}$, we construct a numerical reasoning prompt $\mathcal{P}_{\text{num}}$ (Appendix~\ref{sec:prompts}) and query $N$ LLMs in parallel:
\begin{equation}
\{\mathbf{r}_{\text{LLM}}^{(n)}\}_{n=1}^N = \{\text{LLM}_n(\mathcal{P}_{\text{num}}; \theta_n)\}_{n=1}^N
\end{equation}
We ensemble responses by averaging predicted confidence scores: $c_{\text{ensemble}} = \frac{1}{N}\sum_{n=1}^N c_n$ where $c_n$ is extracted from $\mathbf{r}_{\text{LLM}}^{(n)}$. This ensemble provides additional validation of visual anchors through mathematical reasoning, though it incurs computational overhead (thus optional).

\noindent\textbf{Numerical Constraint Generation.} The fused features $\mathbf{F}_{\text{fused}}$ are projected to produce numerical constraints for Task Executor:
\begin{equation}
\label{eq:constraints}
\mathbf{c}_{\text{upper}} = \boldsymbol{\mu}_{\text{anchor}} + 2\boldsymbol{\sigma}_{\text{anchor}}, \quad \mathbf{c}_{\text{lower}} = \boldsymbol{\mu}_{\text{anchor}} - 2\boldsymbol{\sigma}_{\text{anchor}}
\end{equation}
where $\boldsymbol{\mu}_{\text{anchor}}, \boldsymbol{\sigma}_{\text{anchor}}$ are derived from aggregated anchors. These constraints regularize downstream predictions to respect discovered patterns.

\subsection{Multi-Agent Collaboration}
\label{sec:multi_agent}

\noindent\textbf{Motivation.} Rather than training a single monolithic model to handle all aspects of time series analysis, we decompose the workflow into specialized agents that collaborate synergistically. This design is inspired by how human analysts approach complex tasks: first examining data quality, then visualizing patterns, reasoning about numerical constraints, and finally applying domain-specific models. By codifying this expert workflow into coordinated agents, \method achieves both \emph{specialization} (each agent excels at its subtask) and \emph{efficiency} (parallel execution reduces latency).

\noindent\textbf{Agent Definitions and Roles.} The \method framework comprises four core agents, each designed to address specific analytical needs:

\textbf{(1) Data Analyzer Agent} $\mathcal{A}_{\text{DA}}$ performs preprocessing and statistical analysis. Given $\mathbf{X} \in \mathbb{R}^{L \times D}$, it computes:
\begin{equation}
\label{eq:data_analyzer}
\begin{aligned}
\mathbf{F}_{\text{stat}} &= [\bar{\mathbf{x}}, \boldsymbol{\sigma}, \mathbf{x}_{\min}, \mathbf{x}_{\max}, \text{skew}(\mathbf{X}), \text{kurt}(\mathbf{X}), \mathbf{r}_{\rho}] \\
\mathbf{C}_{\text{cov}} &= \frac{1}{L-1}\sum_{t=1}^L (\mathbf{x}_t - \bar{\mathbf{x}})(\mathbf{x}_t - \bar{\mathbf{x}})^\top \in \mathbb{R}^{D \times D} \\
\mathbf{I}_{\text{imp}} &= \|\mathbf{C}_{\text{cov}}\|_1 \in \mathbb{R}^D, \quad \mathcal{D}_{\text{top-k}} = \text{TopK}(\mathbf{I}_{\text{imp}}, k)
\end{aligned}
\end{equation}
where $\mathbf{r}_{\rho}$ denotes autocorrelations at multiple lags, $\mathbf{C}_{\text{cov}}$ is the covariance matrix, $\mathbf{I}_{\text{imp}}$ measures feature importance via $L_1$-norm of covariance, and $\mathcal{D}_{\text{top-k}}$ selects the most informative $k$ features. This reduces computational complexity from $O(LD)$ to $O(Lk)$ while preserving essential information.

\textbf{(2) Visual Anchor Agent} $\mathcal{A}_{\text{VA}}$ generates prediction anchors as detailed in Section~\ref{sec:visual_anchor}. The key innovation is bridging semantic understanding (from VLM visual analysis) with numerical forecasting through structured anchor representations $\mathcal{A}$.

\textbf{(3) Numerologic Adapter Agent} $\mathcal{A}_{\text{NA}}$ performs multimodal fusion (Section~\ref{sec:numerical_reasoning}) to produce refined features $\mathbf{F}_{\text{fused}}$ and constraints $(\mathbf{c}_{\text{lower}}, \mathbf{c}_{\text{upper}})$ that encode both data-driven patterns and anchor-based priors.

\textbf{(4) Task Executor Agent} $\mathcal{A}_{\text{TE}}$ invokes specialized time series models $\mathcal{M}_{\text{task}}$ (e.g., DLinear~\citep{zeng2023transformers}, TimesNet~\citep{wu2023timesnet}) and applies anchor constraints:
\begin{equation}
\label{eq:task_executor}
\begin{aligned}
\tilde{\mathbf{Y}} &= \mathcal{M}_{\text{task}}(\mathbf{X}_{\text{top-k}}; \theta_{\text{task}}) \\
\hat{\mathbf{Y}} &= \text{Clip}(\tilde{\mathbf{Y}}, \mathbf{c}_{\text{lower}}, \mathbf{c}_{\text{upper}}) = \lambda \tilde{\mathbf{Y}} + (1-\lambda) \text{Clamp}(\tilde{\mathbf{Y}}, \mathbf{c}_{\text{lower}}, \mathbf{c}_{\text{upper}})
\end{aligned}
\end{equation}
where $\mathbf{X}_{\text{top-k}} \in \mathbb{R}^{L \times k}$ contains only selected features, $\text{Clamp}$ enforces hard constraints, and $\lambda \in [0,1]$ balances between model flexibility ($\lambda \approx 1$) and anchor adherence ($\lambda \approx 0$). We set $\lambda=0.7$ by default for soft regularization.

\noindent\textbf{Parallel Execution Strategy.} The Manager Agent $\mathcal{A}_{\text{M}}$ orchestrates workflow execution by constructing a directed acyclic graph (DAG) of agent dependencies. Agents without dependencies execute in parallel:
\begin{equation}
\mathcal{G} = (\mathcal{V}, \mathcal{E}), \quad \mathcal{V} = \{\mathcal{A}_{\text{DA}}, \mathcal{A}_{\text{VA}}, \mathcal{A}_{\text{NA}}, \mathcal{A}_{\text{TE}}\}
\end{equation}
where edges $\mathcal{E}$ represent data dependencies (e.g., $\mathcal{A}_{\text{DA}} \rightarrow \mathcal{A}_{\text{VA}}$ since visual anchoring requires statistical context). Agents at the same DAG level execute concurrently using asynchronous programming, reducing wall-clock time from $O(\sum_{i} t_i)$ to $O(\max_{i \in \text{level}} t_i)$ where $t_i$ is agent $i$'s execution time.

\noindent\textbf{Batch-Parallel Optimization.} For large batches ($B > 8$), the Manager further splits data into $M$ sub-batches $\{\mathbf{X}^{(m)}\}_{m=1}^M$ and processes them concurrently:
\begin{equation}
\{\hat{\mathbf{Y}}^{(m)}\}_{m=1}^M = \text{ParallelMap}(\text{AgentPipeline}, \{\mathbf{X}^{(m)}\}_{m=1}^M), \quad \hat{\mathbf{Y}} = \text{Concat}(\{\hat{\mathbf{Y}}^{(m)}\}_{m=1}^M)
\end{equation}
This achieves near-linear speedup (2-4$\times$ for typical batch sizes), crucial for real-time applications.

\subsection{Task-Specific Optimization}
\label{sec:task_optimization}

\noindent\textbf{Forecasting.} The model minimizes mean squared error with anchor regularization:
\begin{equation}
\label{eq:loss_forecast}
\begin{aligned}
\mathcal{L}_{\text{forecast}} &= \frac{1}{BHD}\sum_{b=1}^B\sum_{h=1}^H\sum_{d=1}^D \|\hat{y}_{b,h,d} - y_{b,h,d}\|^2 \\
&\quad + \beta \cdot \mathbb{I}[\hat{y}_{b,h,d} \notin [\mathbf{l}_{h,d}, \mathbf{u}_{h,d}]] \cdot \|\hat{y}_{b,h,d} - \text{proj}(\hat{y}_{b,h,d})\|^2
\end{aligned}
\end{equation}
where $\mathbb{I}[\cdot]$ is the indicator function, $\text{proj}(\hat{y}) = \max(\mathbf{l}, \min(\hat{y}, \mathbf{u}))$ projects to the confidence interval, and $\beta > 0$ penalizes violations. This soft constraint encourages predictions to respect anchors while allowing flexibility.

\noindent\textbf{Classification.} We use cross-entropy loss with optional anchor-based regularization on intermediate representations:
\begin{equation}
\label{eq:loss_classification}
\mathcal{L}_{\text{class}} = -\frac{1}{B}\sum_{b=1}^B \log p(y_b | \mathbf{X}_b; \theta) + \gamma \|\mathbf{F}_{\text{fused}} - \mathbf{F}_{\text{proto}}\|^2
\end{equation}
where $\mathbf{F}_{\text{proto}}$ are class prototypes learned from visual anchors (discriminative features like periodicity, peak counts), and $\gamma$ weights prototype alignment.

\noindent\textbf{Imputation.} We minimize reconstruction error on masked positions $\mathcal{M}$ with anchor consistency:
\begin{equation}
\label{eq:loss_imputation}
\mathcal{L}_{\text{impute}} = \frac{1}{|\mathcal{M}|}\sum_{(i,j) \in \mathcal{M}} \|\hat{x}_{i,j} - x_{i,j}\|^2 + \eta \cdot \text{Var}(\hat{\mathbf{X}}) - \text{Var}(\mathbf{X}_{\text{anchor}})
\end{equation}
where the variance regularization term ensures imputations match anchor-predicted patterns.

\noindent\textbf{Anomaly Detection.} We compute anomaly scores via reconstruction error and statistical deviation:
\begin{equation}
\label{eq:anomaly_score}
s_t = \omega_1 \|\mathbf{x}_t - \hat{\mathbf{x}}_t\|^2 + \omega_2 \cdot \frac{\|\mathbf{x}_t - \bar{\mathbf{x}}\|}{\boldsymbol{\sigma}} + \omega_3 \cdot \mathbb{I}[\mathbf{x}_t \notin \text{Anchors}]
\end{equation}
where $\omega_i$ are learned weights, the second term is z-score deviation (from Data Analyzer), and the third term flags anchor violations (from Visual Anchor).
