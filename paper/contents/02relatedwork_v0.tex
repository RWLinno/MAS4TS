\section{Related Work}

\noindent\textbf{Deep Models for Time Series Forecasting.} Deep learning models have demonstrated strong performance in time series forecasting. Among these, LSTMs are widely used for sequence modeling~\cite{hochreiter1997long}, while temporal convolutional networks~\cite{bai2018empirical} and Transformers~\cite{wen2023transformers} handle longer temporal dependencies. However, Transformers' self-attention mechanisms often introduce high computational and memory costs. To address this, models such as LogTrans \cite{li2019logtrans}, Reformer \cite{kitaev2020reformer}, Informer \cite{zhou2021informer}, and Pyraformer \cite{liu2022pyraformer} have been proposed to improve efficiency. For example, PatchTST~\cite{nie2022time} segments time series into patches, treating them as input tokens to the Transformer thereby reducing computational complexity while enhancing long-term forecasting accuracy. Another line of research focuses on capturing complex temporal patterns. For example, Autoformer \cite{wu2021autoformer} use a decomposition architecture with auto-correlation to progressively capture dependencies in complex time series data. Similarly, TSformer \cite{woo2022etsformer} incorporates exponential smoothing principles and frequency attention to replace standard self-attention. FEDformer \cite{zhou2022fedformer} combines Transformers with seasonal-trend decomposition, using frequency enhancement to better handle long-term predictions, while NSformer \cite{liu2022non} compensates for non-stationary information to boost performance. While these models perform well on specific tasks, their designs are often tailored to narrow domains, limiting their generalizability across diverse time series data.

\noindent\textbf{Language Models for Time Series Forecasting.} Recent advances in natural language processing (NLP) have led to the exploration of large language models (LLMs) for time series forecasting, thereby by eliminating the need for costly training from scratch \cite{devlin2018bert,brown2020language,touvron2023llama}. For instance, LLMTime~\cite{gruver2023large} adapts LLMs for zero-shot forecasting by tokenizing time series data for autoregressive generation, while LLM4TS~\cite{chang2023llm4ts} uses a two-stage fine-tuning process, starting with pre-training on time series data followed by task-specific fine-tuning. Other approaches adapt time series to language model on input and output to complete input and prediction, such as GPT4TS \cite{zhou2023one} and TimeLLM~\cite{jin2023time}, reprogram time series data into textual representations for LLMs to process, leveraging their reasoning capabilities. Besides, UniTime \cite{liu2024unitime} enhances cross-domain forecasting by integrating a masking mechanism, domain-specific instructions and length-adjusted prediction header, while TimeFFM~\cite{liu2024time} introduces a decentralized federated learning model for time series prediction. Despite the advantages of LLMs, their reliance on textual representations can lead to information loss, as time series data are inherently continuous and multivariate, which is difficult to fully capture in text-based formats. Additionally, these methods suffer from limited interpretability, high computational costs, and simplified embeddings that may inadequately represent complex time series patterns.

\noindent\textbf{Visual-based Time Series Forecasting.} In contrast to LLM-based approaches, visual models have recently been adapted for time series forecasting by converting time series data into image-like representations. For example, some methods transform time series into images and then use convolutional encoders (e.g., SBoF or CNN) to extract features, which are then combined with multiple models for prediction \cite{li2020forecasting, sood2021visual}. TimesNet \cite{wu2023timesnet} decomposes time series into two-dimensional tensors by frequency to capture intra- and inter-cycle changes, using two-dimensional convolutions (e.g., ResNet \cite{he2016deep}, ResNeXt \cite{xie2017aggregated}, ConvNeXt \cite{liu2022convnet}) to extract these features effectively. VisionTS \cite{chen2024visiontsvisualmaskedautoencoders} splits time series into a two-dimensional matrix and renders it as a grayscale image to match pre-trained visual models (e.g., MAE \cite{MaskedAutoencoders2021}), enabling zero-shot or few-shot forecasting. TimeMixer++ \cite{wang2024timemixer++} applies multi-scale and multi-resolution techniques, converting time series into multi-resolution time images to capture complex patterns in both time and frequency domains. These methods show that transforming time series into visual formats enables natural alignment with visual models, leveraging the shared continuity and information density of time series and images. Building on this, we explore how visual representations can bridge the gap between time series and text. \method thus introduces a novel multimodal framework that combines strengths from both visual and LLM-based approaches, establishing a new paradigm for time series forecasting.

\noindent\textbf{Vision-Language Models.} Vision-Language Models (VLMs) have emerged as powerful tools for jointly processing visual and textual data, learning rich multimodal representations. Models like CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling} align images with textual descriptions, demonstrating strong performance across tasks such as image captioning and visual question answering. Early improvements to CLIP, such as those in BLIP~\cite{li2022blip} and BLIP-2~\cite{li2022blip2}, made it more adaptable to diverse datasets, showing effectiveness in fundamental visual tasks but facing challenges with more complex tasks like visual question answering. Recent advancements, including MiniGPT-4~\cite{chen2023minigpt}, LLaVA~\cite{liu2023visual}, and InstructBLIP~\cite{instructblip}, have expanded model parameters and training datasets to handle more complex tasks. While these models have demonstrated strong performance in multimodal tasks, there remains limited exploration of their application to time series forecasting. In this work, we explore how VLMs can bridge the modality gap between time series and text by transforming time series into visual representations. Our method leverages the strengths of VLMs to enhance time series forecasting by capturing both fine-grained temporal patterns and contextual information, ultimately improving forecasting accuracy.