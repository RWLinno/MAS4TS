\begin{abstract}
Large language models (LLMs) have shown promise in time series forecasting, yet their reliance on discrete textual tokens often leads to information loss, especially for fine-grained temporal features. Recent studies suggest that time series align more naturally with visual representations, which better preserve nuanced patterns. Motivated by this, we propose \method, a unified framework that integrates temporal, visual, and textual modalities for enhanced time series forecasting. Our approach comprises three key components: (1) a Retrieval-Augmented Learner that extracts enriched temporal features through memory bank interactions, (2) a Vision-Augmented Learner that encodes time series as informative three-channel images, and (3) a Text-Augmented Learner that generates contextual textual descriptions. The transformed image-text pairs are processed by a frozen Vision-Language Model (VLM) to generate multimodal embeddings, which are fused with temporal memory bank features via a gated mechanism for final prediction. Extensive experiments demonstrate that \method achieves state-of-the-art performance, establishing a new direction for multimodal time series forecasting. Our code is available at \url{https://anonymous.4open.science/status/Time-VLM}.
\end{abstract}

% Recent work in time series forecasting has explored augmenting models with text or vision modalities to improve accuracy. The major insight is to leverage text for contextual understanding and vision for capturing fine-grained temporal patterns. However, text-augmented models often lose fine-grained details, while vision-augmented models lack semantic context, limiting their complementary potential. To address this, we propose \method, a multimodal framework that uses pre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and textual modalities for enhanced forecasting. Our approach comprises three key components: (1) a Retrieval-Augmented Learner that extracts enriched temporal features through memory bank interactions, (2) a Vision-Augmented Learner that encodes time series as informative images, and (3) a Text-Augmented Learner that generates contextual textual descriptions. These components collaborate with frozen VLMs to produce multimodal embeddings, which are fused with temporal features for final prediction. Extensive experiments demonstrate that \method achieves state-of-the-art performance, establishing a new direction for multimodal time series forecasting. Our code is available at \url{https://anonymous.4open.science/status/Time-VLM}.
